{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_acl_24 = pd.read_csv(\"dataset/df_data_preprocessed.csv\")\n"
      ],
      "metadata": {
        "id": "FW33geuNdIjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install accelerate -U\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWVRznD8u8yU",
        "outputId": "7a81c9ea-82b6-4bdd-87b1-828b8d4c8c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/330.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch\n",
        "\n",
        "df['generated_future_work'] = ''\n",
        "\n",
        "# Assuming df2 already has the necessary columns\n",
        "df['input_text'] = df_sample['Abstract'] + ' ' + df_sample['Introduction'] + ' ' + df_sample['Conclusion']\n",
        "data = df[['input_text', 'Extracted Future Work']]\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)  # 70% training, 30% testing\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "class T5Dataset(Dataset):\n",
        "    def __init__(self, tokenizer, data, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        input_text = row['input_text']\n",
        "        target_text = row['Extracted Future Work']\n",
        "\n",
        "        source_encodings = self.tokenizer(\n",
        "            input_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        target_encodings = self.tokenizer(\n",
        "            target_text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': source_encodings['input_ids'].squeeze(0),  # remove batch dimension\n",
        "            'attention_mask': source_encodings['attention_mask'].squeeze(0),\n",
        "            'labels': target_encodings['input_ids'].squeeze(0)\n",
        "        }\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=300,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "train_dataset = T5Dataset(tokenizer, train_data)\n",
        "test_dataset = T5Dataset(tokenizer, test_data)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "def generate_limitations(text):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512).to(device)\n",
        "    outputs = model.generate(inputs, max_length=150, num_beams=5, early_stopping=True)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "test_data['t5'] = test_data['input_text'].apply(generate_limitations)\n"
      ],
      "metadata": {
        "id": "6Jp-FZ1LdTkX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}