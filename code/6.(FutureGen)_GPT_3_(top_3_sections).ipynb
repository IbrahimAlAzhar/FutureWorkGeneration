{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"dataset/df_data_preprocessed.csv\")"
      ],
      "metadata": {
        "id": "FW33geuNdIjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the average word count\n",
        "df['Word Count'] = pd.to_numeric(df['Word Count'], errors='coerce')\n",
        "\n",
        "average_word_count = df['Word Count'].mean()\n",
        "\n",
        "print(f\"Average Word Count: {average_word_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b25e13c2-52a7-4c4b-e679-a70b4d7053d3",
        "id": "Lzsr1BkI5vvb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Word Count: 67.6675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install transformers"
      ],
      "metadata": {
        "id": "KSnqZ9se5vvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# truncate the token limit to 10,000 (gpt 4 can take 128k, but it costs huge)\n",
        "def truncate_row_to_limit_sentence(row, columns, max_tokens=12000):\n",
        "    accumulated_token_count = 0\n",
        "    last_full_column = None\n",
        "\n",
        "    for column in columns:\n",
        "        text = row[column]\n",
        "        if pd.isna(text):\n",
        "            row[column] = \"\"\n",
        "            continue\n",
        "        # Split text into sentences\n",
        "        sentences = text.split('.')\n",
        "        sentences = [sentence.strip() + '.' for sentence in sentences if sentence.strip() != '']\n",
        "        new_text = []\n",
        "        for sentence in sentences:\n",
        "            tokens = tokenizer.tokenize(sentence)\n",
        "            token_count = len(tokens)\n",
        "            if accumulated_token_count + token_count > max_tokens:\n",
        "                # If adding this sentence exceeds the max, truncate here\n",
        "                row[column] = ' '.join(new_text)\n",
        "                return row  # Stop processing further columns and sentences\n",
        "            else:\n",
        "                new_text.append(sentence)\n",
        "                accumulated_token_count += token_count\n",
        "\n",
        "        # Update the column with all sentences that fit\n",
        "        row[column] = ' '.join(new_text)\n",
        "        last_full_column = column\n",
        "\n",
        "    # If all text fits without exceeding the limit\n",
        "    if last_full_column:\n",
        "        last_index = columns.index(last_full_column) + 1\n",
        "        # Clear out all text beyond the last full column processed\n",
        "        for column in columns[last_index:]:\n",
        "            row[column] = \"\"\n",
        "\n",
        "    return row\n",
        "\n",
        "# Columns to process\n",
        "columns_to_process = ['Abstract', 'Introduction', 'Conclusion']\n",
        "df = df.apply(lambda row: truncate_row_to_limit_sentence(row, columns_to_process), axis=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86dfe8c-d143-4313-d0d2-e12843a5e437",
        "id": "Dg0TK81w5vvc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['response_string'] = df.apply(lambda row: f\"\"\"Abstract: {row['Abstract']}\n",
        "Introduction: {row['Introduction']}\n",
        "Conclusion: {row['Conclusion']}\n",
        "\"\"\", axis=1)\n"
      ],
      "metadata": {
        "id": "0gB8YxiS5vvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84e0cf7a-d5c4-4f1f-93f2-dd07bae5cede",
        "id": "w02n8M1X5vvc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/386.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/325.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "generated_text = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    prompt = \"\"\"You are an AI trained to analyze scientific research and suggest future directions based on the content of a paper.\n",
        "    Below, you will find sections from a scientific article including the 'Abstract', 'Introduction', 'Conclusion' of a scientific paper.\n",
        "    Based on these details, please generate comprehensive and plausible future work suggestions that could extend the research findings,\n",
        "    address limitations, and propose new avenues for exploration.\n",
        "    Generate a future work based on these texts. Future work should be within 100 words. \\n\"\"\" + df['response_string'][i]\n",
        "    summary_text = \"\"\n",
        "\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        # model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0.2  # Adjust the temperature as needed, max_tokens=150\n",
        "    )\n",
        "\n",
        "    for chunk in stream:\n",
        "        summary_chunk = chunk.choices[0].delta.content or \"\"\n",
        "        summary_text += summary_chunk\n",
        "    text_chunks = []\n",
        "    text_chunks.append(summary_text)\n",
        "    generated_text.append(text_chunks)"
      ],
      "metadata": {
        "id": "U5yKcrZLuJL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_generated_future_work = pd.DataFrame(generated_summary, columns=['Future_Work'])\n",
        "df['Future_Work'] = df_generated_future_work['Future_Work']"
      ],
      "metadata": {
        "id": "maSXBWE45vvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('dataset/gpt3.5_3_imp_sections.csv', index=False)  # Set index=False if you don't want to save the row indices"
      ],
      "metadata": {
        "id": "9I74M6jQ5vve"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}