{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### measuring hallucination"
      ],
      "metadata": {
        "id": "3UuoQCon3k0g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qJAYAjU23Qr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "# Function to perform NLI classification via GPT\n",
        "def check_nli(premise: str, hypothesis: str) -> str:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": (\n",
        "            \"You are a natural language inference (NLI) classifier. \"\n",
        "            \"Given a premise and hypothesis, respond with exactly one word: \"\n",
        "            \"'entailment', 'neutral', or 'contradiction'.\"\n",
        "        )},\n",
        "        {\"role\": \"user\", \"content\": f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nLabel:\"}\n",
        "    ]\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip().lower()\n",
        "\n",
        "# Apply NLI check across the DataFrame\n",
        "df_rest['nli_label'] = df_rest.apply(\n",
        "    lambda row: check_nli(row['combined'], row['RAG_generated_fw_from_paper']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Calculate hallucination rate (contradiction or neutral)\n",
        "hallucination_mask = df_rest['nli_label'].isin(['neutral', 'contradiction'])\n",
        "hallucination_count = hallucination_mask.sum()\n",
        "total = len(df_rest)\n",
        "hallucination_rate = hallucination_count / total\n",
        "\n",
        "# Print results\n",
        "print(f\"Total samples: {total}\")\n",
        "print(f\"Hallucinations (neutral or contradiction): {hallucination_count}\")\n",
        "print(f\"Hallucination rate: {hallucination_rate:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### feasability check"
      ],
      "metadata": {
        "id": "RFXtlYxj3FMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "import base64\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Define the OpenAI streaming function for GPT-4o-mini\n",
        "def run_critic_openai(prompt: str):\n",
        "    summary_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        summary_text += chunk.choices[0].delta.content or \"\"\n",
        "    return summary_text.strip()\n",
        "\n",
        "# Now your batch‚Äêprocessing loop:\n",
        "all_generated_summary = []\n",
        "start_time = time.time()"
      ],
      "metadata": {
        "id": "sYYy5DGR3XpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated prompt template including paper context\n",
        "feasibility_prompt_template = (\n",
        "    \"You are an expert reviewer. Below is the content of a research paper followed by a suggestion for future work. \"\n",
        "    \"Evaluate whether the future work is executable in the context of the paper's methodology, dataset, or other components. \"\n",
        "    \"Respond with exactly one word: 'feasible' or 'not feasible'.\\n\\n\"\n",
        "    \"Paper Content:\\n{}\\n\\n\"\n",
        "    \"Future Work Suggestion:\\n{}\"\n",
        ")\n",
        "\n",
        "# Initialize the output column\n",
        "df_neurips[\"feasibility_llm_judge_with_ref\"] = None\n",
        "\n",
        "# Process each row\n",
        "for i in range(len(df_neurips)): # len(df_neurips)\n",
        "    paper_content = df_neurips.at[i, 'combined']\n",
        "    future_work = df_neurips.at[i, 'RAG_generated_fw_from_paper']\n",
        "\n",
        "    # Skip empty or invalid input\n",
        "    if not future_work or not isinstance(future_work, str):\n",
        "        df_neurips.at[i, \"feasibility_llm_judge\"] = \"empty\"\n",
        "        continue\n",
        "\n",
        "    prompt = feasibility_prompt_template.format(paper_content, future_work)\n",
        "\n",
        "    try:\n",
        "        response = run_critic_openai(prompt, retries=2)\n",
        "        label = response.strip().lower()\n",
        "        if label not in [\"feasible\", \"not feasible\"]:\n",
        "            label = \"unknown\"\n",
        "    except Exception as e:\n",
        "        print(f\"[Error] Row {i} - {e}\")\n",
        "        label = \"error\"\n",
        "\n",
        "    df_neurips.at[i, \"feasibility_llm_judge_with_ref\"] = label\n",
        "    # time.sleep(1)  # avoid rate limits\n",
        "\n",
        "\n",
        "feasible_count = (df_neurips['feasibility_llm_judge_with_ref'] == 'feasible').sum()\n",
        "print(f\"Number of rows marked as feasible: {feasible_count}\")\n",
        "\n",
        "total_valid = df_neurips['feasibility_llm_judge_with_ref'].isin(['feasible', 'not feasible']).sum()\n",
        "feasible_rate = feasible_count / total_valid if total_valid else 0\n",
        "print(f\"Feasible rate: {feasible_rate:.2%} ({feasible_count}/{total_valid})\")"
      ],
      "metadata": {
        "id": "eSzPdLu63ELB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}