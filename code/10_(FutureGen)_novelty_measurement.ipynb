{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vhx60ZS2Fp2"
      },
      "outputs": [],
      "source": [
        "novelty_checker_prompt = ''' You are an expert in evaluating research content for novelty and innovation. I have two sets of text\n",
        "provided below:\n",
        "\n",
        "Author-Mentioned Future Work:\n",
        "<Author-Mentioned Future Work>\n",
        "\n",
        "LLM-Generated Future Work:\n",
        "<LLM-Generated Future Work>\n",
        "\n",
        "Your task is to compare the LLM-generated future work to the author-mentioned future work and assess its novelty relative to the\n",
        "author-mentioned future work. Follow these steps:\n",
        "\n",
        "Evaluate Novelty: Identify unique ideas, approaches, or directions in the LLM-generated future work that are not present in\n",
        "the author-mentioned future work. Analyze how innovative or distinct these additions are in the context of the research field.\n",
        "\n",
        "Quantify Novelty: Provide a novelty score (0-10) for the LLM-generated future work, where 0 indicates complete overlap with the\n",
        "author’s future work (no new ideas) and 10 indicates entirely new and distinct ideas. Justify the score with a clear reason explaining\n",
        "the extent of novel contributions or lack thereof.\n",
        "\n",
        "Present your response in a JSON object with the keys score (integer from 0-10) and reason (a concise explanation of the score).\n",
        "Ensure your response is concise, precise, and grounded in the provided texts. If the provided texts are incomplete or unclear,\n",
        "request clarification or specific excerpts to proceed.\n",
        "\n",
        "Example output format: { \"score\": 5, \"reason\": \"The LLM-generated future work introduces a new approach to cross-lingual embeddings,\n",
        "which is not mentioned in the author’s future work, but it also repeats the idea of low-resource language support, reducing its\n",
        "overall novelty.\" }\n",
        "\n",
        "\\n\\n\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "generated_summary = []\n",
        "# abstracts = []\n",
        "\n",
        "for i in range(len(df_rest)): # len(df_rest)\n",
        "    #prompt = \"Objective: Generate summary for the following passage from a scientific paper. Consider the Abstract, Introduction, and other sections of a scientific paper. Each summary should be 200 to 300 words. \\n\\n\" + \"Abstract:\\n\" + df_cleaned['Abstract'][i] + \"\\n\" + \"Introduction:\\n\" + df_cleaned['Introduction'][i] + \"Others:\\n\" + df_cleaned['col_1'][i] + \"\\n\" +  df_cleaned['col_2'][i] + \"\\n\" + df_cleaned['col_3'][i] + \"\\n\" +  df_cleaned['col_4'][i] + \"\\n\" +  df_cleaned['col_5'][i] + \"\\n\" +  df_cleaned['col_6'][i] + \"\\n\" + df_cleaned['col_7'][i] + \"\\n\"\n",
        "    author_str = df_rest.at[i, 'LLM_extracted_future_work']\n",
        "    generated_str = df_rest.at[i, 'RAG_generated_fw_from_paper']\n",
        "\n",
        "    # author_str = \"\\n\".join(author_list)\n",
        "    # generated_str = \"\\n\".join(generated_list)\n",
        "\n",
        "    prompt = (\n",
        "        novelty_checker_prompt\n",
        "        + \"Author-Mentioned Future Work:\\n\"\n",
        "        + author_str\n",
        "        + \"\\n\\nLLM-Generated Future Work:\\n\"\n",
        "        + generated_str\n",
        "    )\n",
        "    # prompt =  novelty_checker_prompt + \"Author-Mentioned Future Work: \" + df_neurips['LLM_extracted_future_work'][i] + \"LLM-Generated Future Work: \" + df_neurips['LLM_generated_future_work_paper_only'][i]\n",
        "\n",
        "\n",
        "    summary_text = \"\"  # Initialize an empty string to collect the limitation text\n",
        "\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0.1 # Adjust the temperature as needed, max_tokens=150\n",
        "    )\n",
        "\n",
        "    for chunk in stream:\n",
        "        summary_chunk = chunk.choices[0].delta.content or \"\"\n",
        "        # print(limitation_chunk, end=\"\")\n",
        "        summary_text += summary_chunk  # Append each chunk to the limitation_text\n",
        "\n",
        "    # print(\"\\n\")  # Print a newline for readability\n",
        "    summary_chunks = []\n",
        "    summary_chunks.append(summary_text)\n",
        "    generated_summary.append(summary_chunks)  # Append the collected limitation text to the list\n"
      ],
      "metadata": {
        "id": "7wfTprHT2Lwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Flatten the list of lists\n",
        "flat = [item[0] for item in generated_summary]\n",
        "\n",
        "# Parse each JSON snippet\n",
        "records = []\n",
        "for snippet in flat:\n",
        "    match = re.search(r\"\\{.*\\}\", snippet, flags=re.S)\n",
        "    if match:\n",
        "        data = json.loads(match.group(0))\n",
        "        records.append(data)\n",
        "\n",
        "# Create DataFrame\n",
        "df_gpt_judge = pd.DataFrame(records)\n"
      ],
      "metadata": {
        "id": "37Vs6wSS2PWh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}