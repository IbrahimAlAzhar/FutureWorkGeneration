{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv1YhU7l-ROY"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"dataset/df_gpt_3.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantitaive Evaluations"
      ],
      "metadata": {
        "id": "HgpFTDGmq_Q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install bert_score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6d0tP05r_Ls",
        "outputId": "eee178a7-1068-40b3-8837-1cc94debe5f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTScore for row by row (roberta-large model)\n",
        "import pandas as pd\n",
        "from bert_score import BERTScorer\n",
        "\n",
        "# Convert each 'Limitation' row to a list containing that single row's string\n",
        "refs = df['GPT_ground_truth_FW'].apply(lambda x: [str(x).strip()]).tolist()\n",
        "cands = df['generated_future_work'].apply(lambda x: [str(x).strip()]).tolist()\n",
        "\n",
        "# Initialize the scorer\n",
        "scorer = BERTScorer(model_type='roberta-large')\n",
        "\n",
        "# Initialize lists to store individual scores and accumulators for averages\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "total_precision, total_recall, total_f1 = 0, 0, 0\n",
        "\n",
        "# Loop through each reference and candidate\n",
        "for ref, cand in zip(refs, cands):\n",
        "    if ref and cand:  # Ensure neither is empty\n",
        "        # Calculate scores\n",
        "        P, R, F1 = scorer.score(cand, ref)\n",
        "\n",
        "        # Convert to scalar values for printing and storing\n",
        "        precision = P.mean().item()\n",
        "        recall = R.mean().item()\n",
        "        f1_score = F1.mean().item()\n",
        "\n",
        "        # Append scores to respective lists\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1_score)\n",
        "\n",
        "        # Update total accumulators\n",
        "        total_precision += precision\n",
        "        total_recall += recall\n",
        "        total_f1 += f1_score\n",
        "\n",
        "        # Print each score along with refs and cands\n",
        "        print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1_score}\")\n",
        "        print(f\"Reference: {ref}\")\n",
        "        print(f\"Candidate: {cand}\\n\")\n",
        "\n",
        "# Calculate averages\n",
        "num_scores = len(precision_scores)\n",
        "average_precision = total_precision / num_scores if num_scores > 0 else 0\n",
        "average_recall = total_recall / num_scores if num_scores > 0 else 0\n",
        "average_f1 = total_f1 / num_scores if num_scores > 0 else 0\n",
        "\n",
        "# Add the individual scores to the DataFrame\n",
        "df['BERTScore_Precision'] = precision_scores\n",
        "df['BERTScore_Recall'] = recall_scores\n",
        "df['BERTScore_F1'] = f1_scores"
      ],
      "metadata": {
        "id": "ej7cqsae8tpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "8FLXEMiouQeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.astype(str)\n",
        "\n",
        "!pip3 -q install evaluate\n",
        "!pip -q install rouge_score\n",
        "\n",
        "def lcs(X, Y):\n",
        "    m = len(X)\n",
        "    n = len(Y)\n",
        "    L = [[0] * (n + 1) for i in range(m + 1)]\n",
        "\n",
        "    for i in range(m + 1):\n",
        "        for j in range(n + 1):\n",
        "            if i == 0 or j == 0:\n",
        "                L[i][j] = 0\n",
        "            elif X[i - 1] == Y[j - 1]:\n",
        "                L[i][j] = L[i - 1][j - 1] + 1\n",
        "            else:\n",
        "                L[i][j] = max(L[i - 1][j], L[i][j - 1])\n",
        "    return L[m][n]\n",
        "\n",
        "def rough_1(generated_summary, reference_summary):\n",
        "    gen_unigrams = set(generated_summary.split())\n",
        "    ref_unigrams = set(reference_summary.split())\n",
        "\n",
        "    common_unigrams = gen_unigrams.intersection(ref_unigrams)\n",
        "    precision = len(common_unigrams) / len(gen_unigrams)\n",
        "    return precision\n",
        "\n",
        "\n",
        "def rough_2(generated_summary, reference_summary):\n",
        "    gen_bigrams = set(zip(generated_summary.split(), generated_summary.split()[1:]))\n",
        "    ref_bigrams = set(zip(reference_summary.split(), reference_summary.split()[1:]))\n",
        "    common_bigrams = gen_bigrams.intersection(ref_bigrams)\n",
        "    precision = len(common_bigrams) / len(gen_bigrams)\n",
        "    return precision\n",
        "\n",
        "total_rough2_score = 0\n",
        "for i in range(len(df)):\n",
        "  rough_2_score = rough_2(df['Future_Work'][i], df['Extracted Future Work'][i])\n",
        "  total_rough2_score += rough_2_score\n",
        "print(\"average_rough_2_score\",total_rough2_score/len(df))\n",
        "\n",
        "total_rough1_score = 0\n",
        "for i in range(len(df)):\n",
        "  rough_1_score = rough_1(df['Future_Work'][i], df['Extracted Future Work'][i])\n",
        "  total_rough1_score += rough_1_score\n",
        "print(\"average_rough_1_score\",total_rough1_score/len(df))\n",
        "\n",
        "# cosine similarity\n",
        "def rough_l(generated_summary, reference_summary):\n",
        "    lcs_length = lcs(generated_summary.split(), reference_summary.split())\n",
        "    precision = lcs_length / len(reference_summary.split())\n",
        "    recall = lcs_length / len(generated_summary.split())\n",
        "\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1_score\n",
        "\n",
        "total_rough_l_score = 0\n",
        "for i in range(len(df)):\n",
        "  rough_l_score = rough_l(df['Future_Work'][i], df['Extracted Future Work'][i])\n",
        "  total_rough_l_score += rough_l_score\n",
        "print(\"average_rough_l_score\",total_rough_l_score/len(df))\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_similarity(text1, text2):\n",
        "    # Use CountVectorizer to convert text to a matrix of token counts\n",
        "    vectorizer = CountVectorizer().fit_transform([text1, text2])\n",
        "\n",
        "    # Calculate cosine similarity between the two texts\n",
        "    similarity_matrix = cosine_similarity(vectorizer)\n",
        "\n",
        "    # The similarity_matrix[0, 1] contains the cosine similarity between text1 and text2\n",
        "    return similarity_matrix[0, 1]\n",
        "\n",
        "def calculate_average_rough_score(text1, text2):\n",
        "    # Calculate similarity scores\n",
        "    similarity_score1 = calculate_similarity(text1, text2)\n",
        "    similarity_score2 = calculate_similarity(text2, text1)\n",
        "\n",
        "    # Calculate average rough score\n",
        "    average_rough_score = (similarity_score1 + similarity_score2) / 2\n",
        "\n",
        "    return average_rough_score\n",
        "\n",
        "total_score = 0  # Initialize a variable to accumulate scores\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "    reference_summary = df['Extracted Future Work'][i]\n",
        "    generated_summary = df['Future_Work'][i]\n",
        "    average_rough_score = calculate_average_rough_score(reference_summary, generated_summary)\n",
        "\n",
        "    total_score += average_rough_score\n",
        "\n",
        "# Calculate the overall average\n",
        "overall_average = total_score / len(df)\n",
        "\n",
        "# Print the overall average\n",
        "print(f\"\\nOverall Average cosine similarity Score: {overall_average}\")\n",
        "\n",
        "def jaccard_similarity(text1, text2):\n",
        "    # Tokenize the texts by splitting on whitespace\n",
        "    set1 = set(text1.split())\n",
        "    set2 = set(text2.split())\n",
        "\n",
        "    # Calculate the intersection and union of the sets\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "\n",
        "    # Compute Jaccard similarity\n",
        "    return len(intersection) / len(union) if union else 0\n",
        "\n",
        "# Calculate Jaccard similarity for each pair of rows df['Extracted Future Work'][i]\n",
        "jaccard_similarities = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    similarity = jaccard_similarity(df.loc[i, 'Extracted Future Work'], df.loc[i, 'Future_Work'])\n",
        "    jaccard_similarities.append(similarity)\n",
        "\n",
        "# Add the Jaccard similarity as a new column in df1\n",
        "df['Jaccard_Similarity'] = jaccard_similarities\n",
        "\n",
        "# Calculate the average Jaccard similarity\n",
        "average_jaccard_similarity = sum(jaccard_similarities) / len(jaccard_similarities)\n",
        "\n",
        "print(\"average jaccard similarity\",average_jaccard_similarity)\n",
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "def calculate_bleu(reference, candidate):\n",
        "    # Tokenize the sentences using nltk's word_tokenize function\n",
        "    reference_tokens = word_tokenize(reference.lower())\n",
        "    candidate_tokens = word_tokenize(candidate.lower())\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    score = sentence_bleu([reference_tokens], candidate_tokens)\n",
        "    return score\n",
        "\n",
        "# First, ensure that both columns are converted to string type and handle any missing values\n",
        "df['Extracted Future Work'] = df['Extracted Future Work'].astype(str)\n",
        "df['Future_Work'] = df['Future_Work'].astype(str)\n",
        "\n",
        "# Calculate BLEU scores using a list comprehension and store in a list\n",
        "bleu_scores = [calculate_bleu(ref, cand) for ref, cand in zip(df['Extracted Future Work'],df['Future_Work'])]\n",
        "\n",
        "# Optional: Store the BLEU scores back into a DataFrame for analysis\n",
        "results_df = pd.DataFrame({'BLEU Score': bleu_scores})\n",
        "\n",
        "\n",
        "# Calculate the average BLEU score\n",
        "average_bleu_score = results_df['BLEU Score'].mean()\n",
        "\n",
        "# Print the average BLEU score\n",
        "print(f\"Average BLEU Score: {average_bleu_score}\")"
      ],
      "metadata": {
        "id": "MrxWE4MF8xWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Judge by LLM"
      ],
      "metadata": {
        "id": "vmFEYLTk_jcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q groq"
      ],
      "metadata": {
        "id": "vxharKg3DaJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad62170-242e-40a3-9067-662467e33e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/106.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "prompt1 = \"\"\"Title: Evaluation of Text Quality\n",
        "\n",
        "Instructions:\n",
        "\n",
        "You are provided with two texts for each pair: one is generated by a machine (Machine-Generated Text), and the other is the original or ground truth text (Ground Truth). Please read both texts carefully. After reviewing each text, assign a score from 1 to 5 based on the criteria outlined below. The score should reflect how well the machine-generated text compares to the ground truth, where 1 represents poor quality and 5 represents excellent quality that closely matches or even surpasses the ground truth in some aspects.\n",
        "\n",
        "Scoring Criteria:\n",
        "Coherence and Logic:\n",
        "\n",
        "5: The text is exceptionally coherent; the ideas flow logically and are well connected.\n",
        "3: The text is coherent but may have occasional lapses in logic or flow.\n",
        "1: The text is disjointed or frequently illogical.\n",
        "Relevance and Accuracy:\n",
        "\n",
        "5: The text is completely relevant to the topic and accurate in all presented facts.\n",
        "3: The text is generally relevant with minor factual errors or slight deviations from the topic.\n",
        "1: The text often strays off topic or includes multiple factual inaccuracies.\n",
        "Readability and Style:\n",
        "\n",
        "5: The text is engaging, well-written, and stylistically consistent with the ground truth.\n",
        "3: The text is readable but may lack flair or have minor stylistic inconsistencies.\n",
        "1: The text is difficult to read or stylistically poor.\n",
        "Grammatical Correctness:\n",
        "\n",
        "5: The text is free from grammatical errors.\n",
        "3: The text has occasional grammatical errors that do not impede understanding.\n",
        "1: The text has frequent grammatical errors that hinder comprehension.\n",
        "Overall Impression:\n",
        "\n",
        "5: The text is of a quality that you would expect from a professional writer.\n",
        "3: The text is acceptable but would benefit from further editing.\n",
        "1: The text is of a quality that needs significant revision to be usable.\n",
        "Task:\n",
        "\n",
        "For each text pair:\n",
        "\n",
        "Rate the Machine-Generated Text on each criterion and provide a final overall score out of 5.\n",
        "Provide a brief justification for your scores, highlighting strengths and weaknesses observed in the machine-generated text relative to the ground truth.\n",
        "Example of Usage:\n",
        "Text Pair 1:\n",
        "\n",
        "Machine-Generated Text: \"The quick brown fox jumps over the lazy dog repeatedly.\"\n",
        "\n",
        "Ground Truth: \"A quick brown fox consistently jumps over the lazy dog.\"\n",
        "\n",
        "Evaluation:\n",
        "\n",
        "Coherence and Logic: 5\n",
        "Relevance and Accuracy: 4\n",
        "Readability and Style: 5\n",
        "Grammatical Correctness: 5\n",
        "Overall Impression: 5\n",
        "Justification: The machine-generated text maintains the core message and style of the ground truth, presenting it coherently and engagingly. Minor variations in wording do not impact the overall quality or relevance of the message. \"\"\""
      ],
      "metadata": {
        "id": "BVzURy__DaJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(\n",
        "    api_key=userdata.get('GROQ_API_KEY'),\n",
        ")"
      ],
      "metadata": {
        "id": "jv_N6TZwDaJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluating by Llama"
      ],
      "metadata": {
        "id": "2JY484AzA51T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LLM_feedback = []\n",
        "\n",
        "# Assuming prompt1 is defined and properly configured\n",
        "for i in range(len(df)):\n",
        "  prompt_2 = f\"Ground Truth: {df['Extracted Future Work'][i]} Machine Generate Text: {df['Future_Work'][i]}\"\n",
        "\n",
        "  chat_completion = client.chat.completions.create(\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": prompt1},\n",
        "      {\"role\": \"user\", \"content\": prompt_2}\n",
        "      ],\n",
        "      model=\"llama3-70b-8192\",\n",
        "      temperature=0.2,\n",
        "      stream=False,\n",
        "    )\n",
        "  feedback_text = chat_completion.choices[0].message.content or \"\"\n",
        "  LLM_feedback.append([feedback_text])\n"
      ],
      "metadata": {
        "id": "EIk78UFqtR8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluating by gpt 3.5"
      ],
      "metadata": {
        "id": "QpfXZO7IBITk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "LLM_feedback = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    prompt2 = f\"Ground Truth: {df['GPT_ground_truth_FW'][i]} Machine Generate Text: {df['Future_Work'][i]}\"\n",
        "    prompt = prompt1 + prompt2\n",
        "    summary_text = \"\"  # Initialize an empty string to collect the limitation text\n",
        "\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        # model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0.2\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        feedback_chunk = chunk.choices[0].delta.content or \"\"\n",
        "        feedback_text += feedback_chunk\n",
        "\n",
        "    feedback_chunks = []\n",
        "    feedback_chunks.append(feedback_text)\n",
        "    LLM_feedback.append(feedback_chunks)"
      ],
      "metadata": {
        "id": "trjz0o8ZA8c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_LLM_feedback = pd.DataFrame(LLM_feedback, columns=['LLM_feedback'])"
      ],
      "metadata": {
        "id": "WrgQT-GEWNg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to extract scores from each category\n",
        "def extract_score(text, category):\n",
        "    pattern = rf\"{category}: (\\d)\"\n",
        "    score = re.search(pattern, text)\n",
        "    return int(score.group(1)) if score else np.nan\n",
        "\n",
        "# Apply the function for each category\n",
        "df_LLM_feedback['Coherence and Logic'] = df_LLM_feedback['LLM_feedback'].apply(extract_score, category='Coherence and Logic')\n",
        "df_LLM_feedback['Relevance and Accuracy'] = df_LLM_feedback['LLM_feedback'].apply(extract_score, category='Relevance and Accuracy')\n",
        "df_LLM_feedback['Readability and Style'] = df_LLM_feedback['LLM_feedback'].apply(extract_score, category='Readability and Style')\n",
        "df_LLM_feedback['Grammatical Correctness'] = df_LLM_feedback['LLM_feedback'].apply(extract_score, category='Grammatical Correctness')\n",
        "df_LLM_feedback['Overall Impression'] = df_LLM_feedback['LLM_feedback'].apply(extract_score, category='Overall Impression')\n",
        "\n",
        "# Calculate average for each category\n",
        "average_coherence_logic = df_LLM_feedback['Coherence and Logic'].mean()\n",
        "average_relevance_accuracy = df_LLM_feedback['Relevance and Accuracy'].mean()\n",
        "average_readability_style = df_LLM_feedback['Readability and Style'].mean()\n",
        "average_grammatical_correctness = df_LLM_feedback['Grammatical Correctness'].mean()\n",
        "average_overall_impression = df_LLM_feedback['Overall Impression'].mean()\n",
        "\n",
        "# (average_coherence_logic, average_relevance_accuracy, average_readability_style, average_grammatical_correctness, average_overall_impression)\n",
        "print(\"average_coherence_logic\",average_coherence_logic)\n",
        "print(\"average_relevance_accuracy\",average_relevance_accuracy)\n",
        "print(\"average_readability_style\",average_readability_style)\n",
        "print(\"average_overall_impression\",average_overall_impression)\n"
      ],
      "metadata": {
        "id": "TKaEQY_6AnrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_LLM_feedback.to_csv('dataset/df_LLM_feedback.csv', index=False)  # Set index=False if you don't want to save the row indices"
      ],
      "metadata": {
        "id": "938txp8Yeyy5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}