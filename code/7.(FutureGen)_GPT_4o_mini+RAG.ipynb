{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Top 3 sections"
      ],
      "metadata": {
        "id": "izgdNJBGvlsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"dataset/df_data_preprocessed.csv\")"
      ],
      "metadata": {
        "id": "Hq3Ex9taLqcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install transformers"
      ],
      "metadata": {
        "id": "FBl0liVCLqcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# truncate the token limit to 10,000 (gpt 4 can take 128k, but it costs huge)\n",
        "def truncate_row_to_limit_sentence(row, columns, max_tokens=12000):\n",
        "    accumulated_token_count = 0\n",
        "    last_full_column = None\n",
        "\n",
        "    for column in columns:\n",
        "        text = row[column]\n",
        "        if pd.isna(text):\n",
        "            row[column] = \"\"\n",
        "            continue\n",
        "        # Split text into sentences\n",
        "        sentences = text.split('.')\n",
        "        sentences = [sentence.strip() + '.' for sentence in sentences if sentence.strip() != '']\n",
        "        new_text = []\n",
        "        for sentence in sentences:\n",
        "            tokens = tokenizer.tokenize(sentence)\n",
        "            token_count = len(tokens)\n",
        "            if accumulated_token_count + token_count > max_tokens:\n",
        "                # If adding this sentence exceeds the max, truncate here\n",
        "                row[column] = ' '.join(new_text)\n",
        "                return row  # Stop processing further columns and sentences\n",
        "            else:\n",
        "                new_text.append(sentence)\n",
        "                accumulated_token_count += token_count\n",
        "\n",
        "        # Update the column with all sentences that fit\n",
        "        row[column] = ' '.join(new_text)\n",
        "        last_full_column = column\n",
        "\n",
        "    # If all text fits without exceeding the limit\n",
        "    if last_full_column:\n",
        "        last_index = columns.index(last_full_column) + 1\n",
        "        # Clear out all text beyond the last full column processed\n",
        "        for column in columns[last_index:]:\n",
        "            row[column] = \"\"\n",
        "\n",
        "    return row\n",
        "\n",
        "# Columns to process\n",
        "columns_to_process = ['Abstract', 'Introduction', 'Conclusion']\n",
        "df = df.apply(lambda row: truncate_row_to_limit_sentence(row, columns_to_process), axis=1)\n"
      ],
      "metadata": {
        "id": "07SQX3IPv2ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['response_string'] = df.apply(lambda row: f\"\"\"Abstract: {row['Abstract']}\n",
        "Introduction: {row['Introduction']}\n",
        "Conclusion: {row['Conclusion']}\n",
        "\"\"\", axis=1)"
      ],
      "metadata": {
        "id": "WtXMMjyzLqcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install openai"
      ],
      "metadata": {
        "id": "7vVeQD5cxIS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG + gpt 3.5 turbo"
      ],
      "metadata": {
        "id": "mXJTdBuKMNzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index\n",
        "!pip install -q openai\n",
        "!pip install -q transformers\n",
        "!pip install -q accelerate\n",
        "!pip -q install llama-index-core\n",
        "!pip -q install llama-index-llms-openai\n",
        "!pip -q install llama-index-llms-replicate\n",
        "!pip -q install llama-index-embeddings-huggingface"
      ],
      "metadata": {
        "id": "9atrZA-9xJuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Sample 100 random rows from df for the training set\n",
        "df_rag_train = df.sample(n=100, random_state=42)  # Use a fixed random state for reproducibility\n",
        "\n",
        "# Create the testing set by dropping the sampled rows from df2\n",
        "df_rag_test = df.drop(df_rag_train.index)\n",
        "\n",
        "# Now df_rag_train contains 100 random samples, and df2_test contains the rest\n",
        "df_rag_train.to_csv('dataset/future_work_rag/df_rag_train_fw.csv', index=False)\n",
        "df_rag_test.to_csv('dataset/df_rag_test_fw.csv', index=False)"
      ],
      "metadata": {
        "id": "E7J9aC64MXQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llms import LLM\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core import Settings\n",
        "from IPython.display import Markdown, display\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "from llama_index.core import ServiceContext, set_global_service_context\n",
        "from llama_index.llms import openai\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "\n",
        "documents = SimpleDirectoryReader(\"dataset/future_work_rag\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "# reset index\n",
        "df_rag_test.reset_index(inplace=True, drop=False)\n",
        "\n",
        "df_rag_test['response_string'] = df_rag_test.apply(lambda row: f\"\"\"Abstract: {row['Abstract']}\n",
        "Introduction: {row['Introduction']}\n",
        "Conclusion: {row['Conclusion']}\n",
        "\"\"\", axis=1)\n",
        "\n",
        "index.storage_context.persist()\n",
        "\n",
        "\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
        "index = load_index_from_storage(storage_context=storage_context)\n",
        "\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ],
      "metadata": {
        "id": "qdmqOFIKSVgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
        "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
        "Settings.num_output = 512\n",
        "Settings.context_window = 3900\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "system_prompt = \"\"\"You are an AI trained to analyze scientific research and suggest future directions based on the content of a paper.\n",
        "    Below, you will find sections from a scientific article including the 'Abstract', 'Introduction', 'Conclusion' of a scientific paper.\n",
        "    Based on these details, please generate comprehensive and plausible future work suggestions that could extend the research findings,\n",
        "    address limitations, and propose new avenues for exploration.\n",
        "    Generate a future work based on these texts. Future work should be within 100 words. \\n\"\"\"\n",
        "\n",
        "import io\n",
        "import sys\n",
        "\n",
        "generated_limitations = []\n",
        "\n",
        "for i in range(len(df_rag_test)):  # Assuming you need to iterate from 11 to 11, which is effectively just once\n",
        "    query_engine = index.as_query_engine(\n",
        "        similarity_top_k=3,\n",
        "        streaming=True,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = query_engine.query(\n",
        "            system_prompt + \": \" + df_rag_test['response_string'][i]\n",
        "        )\n",
        "        limitation_text = \"\"\n",
        "        # Redirect stdout to capture print outputs\n",
        "        old_stdout = sys.stdout  # Memorize the default stdout stream\n",
        "        sys.stdout = buffer = io.StringIO()\n",
        "\n",
        "        try:\n",
        "            response.print_response_stream()\n",
        "            limitation_text = buffer.getvalue()  # Get whatever was printed to the \"fake\" stdout\n",
        "        finally:\n",
        "            sys.stdout = old_stdout  # Restore stdout. Important to do this early\n",
        "\n",
        "        generated_limitations.append(limitation_text.strip())\n",
        "\n",
        "    except ValueError as e:\n",
        "        if \"Calculated available context size -418 was not non-negative\" in str(e):\n",
        "            print(\"Caught a ValueError due to negative context size: \", str(e))\n",
        "        else:\n",
        "            raise"
      ],
      "metadata": {
        "id": "NKV3yDHMwEkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_generated_future_work = pd.DataFrame(generated_limitations, columns=['Future_Work'])\n",
        "df_rag_test['generated_future_work'] = df_generated_future_work['Future_Work']"
      ],
      "metadata": {
        "id": "BGes6vOOclsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rag_test.to_csv('df_rag_future_work_gpt_4o_3_imp_sections.csv', index=False)"
      ],
      "metadata": {
        "id": "ygNYGkzFneaA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}