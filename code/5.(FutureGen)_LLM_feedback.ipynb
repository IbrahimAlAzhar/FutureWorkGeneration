{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"dataset/df_gpt3.csv\")\n",
        "df = pd.read_csv(\"dataset/df_LLM_feedback.csv\")\n",
        "df = pd.concat([df3, df4], axis=1)"
      ],
      "metadata": {
        "id": "FxYVO5yzDVih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1a6e07-c1ea-4830-95fa-fa2f80bed74b",
        "id": "YQIhyd2xeO0n"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/386.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m358.4/386.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean up the text\n",
        "def clean_and_extract_justification(text):\n",
        "    # Remove all newline characters\n",
        "    text = text.replace('\\n', ' ')\n",
        "    # Find the start of \"Justification\" and remove everything before it\n",
        "    justification_start = text.find(\"Justification\")\n",
        "    if justification_start != -1:\n",
        "        return text[justification_start:]  # Keep text from \"Justification\" onward\n",
        "    else:\n",
        "        return text  # Return the original text if \"Justification\" is not found\n",
        "\n",
        "# Apply the function to the 'LLM_feedback' column\n",
        "df['Refined_LLM_feedback'] = df['LLM_feedback'].apply(clean_and_extract_justification)\n"
      ],
      "metadata": {
        "id": "ZJSYZT2OjuYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# take feedback which doesn't pass the threshold and send to the LLM to generate refined 'future work' (iteraton 2)"
      ],
      "metadata": {
        "id": "7MIqu_iAE5Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now use the model to send the problem you found and generate 'refined' limitaions\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "generated_limiations = []\n",
        "# abstracts = []\n",
        "# Abstract\tIntroduction conclusion\tRelated work\tmethodology\t\texperiment and results\textra\n",
        "for i in range(len(df)):\n",
        "    coherence_score = df['Coherence and Logic'][i]\n",
        "    relevance_score = df['Relevance and Accuracy'][i]\n",
        "    readability_score = df['Readability and Style'][i]\n",
        "    grammar_score = df['Grammatical Correctness'][i]\n",
        "    overall_score = df['Overall Impression'][i]\n",
        "    bscore_f1 = df['BERTScore_F1'][i]\n",
        "\n",
        "    if coherence_score > 3 and relevance_score > 3 and readability_score > 3 and grammar_score > 3 and overall_score > 3 and bscore_f1 > 0.85:\n",
        "      generated_limiations.append([df['Future_Work'][i]])\n",
        "\n",
        "    else:\n",
        "      print(\"else\",i)\n",
        "      prompt_template = \"\"\"You are an AI model trained to analyze scientific content and provide high-quality future research suggestions. Below, you will find sections\n",
        "      from a scientific article, along with a justification for improving the \"Future Work\" section. Please use the article content and justification to generate an enhanced\n",
        "      \"Future Work\" section that addresses the limitations mentioned.\n",
        "\n",
        "      Your goal is to create a cohesive, logically structured, and engaging \"Future Work\" section that aligns closely with the article's content.\n",
        "      The suggestion should also consider areas of improvement outlined in the justification, such as Coherence and Logic, Relevance and Accuracy, Readability and Style,\n",
        "      Grammatical Correctness, and precision. Aim for a concise response within 100 words.\n",
        "\n",
        "      Article Content:\n",
        "      Abstract:\n",
        "      \"{abstract_text}\"\n",
        "\n",
        "      Introduction:\n",
        "      \"{introduction_text}\"\n",
        "\n",
        "      Conclusion:\n",
        "      \"{conclusion_text}\"\n",
        "\n",
        "      Justification:\n",
        "      \"{justification_text}\"\n",
        "\n",
        "      Based on the article content and the justification provided, generate an improved \"Future Work\" section that builds on the strengths of the original while addressing\n",
        "      the noted weaknesses. Ensure your response is aligned with the main topic and presents clear directions for future research.\n",
        "      \"\"\"\n",
        "\n",
        "      # Replace placeholders with the actual content from the DataFrame\n",
        "      abstract_text = df.loc[i, 'Abstract']  # Replace 0 with the actual row index if needed\n",
        "      introduction_text = df.loc[i, 'Introduction']\n",
        "      conclusion_text = df.loc[i, 'Conclusion']\n",
        "      justification_text = df.loc[i, 'Refined_LLM_feedback']\n",
        "\n",
        "      # Format the final prompt\n",
        "      prompt = prompt_template.format(\n",
        "          abstract_text=abstract_text,\n",
        "          introduction_text=introduction_text,\n",
        "          conclusion_text=conclusion_text,\n",
        "          justification_text=justification_text\n",
        "      )\n",
        "\n",
        "      summary_text = \"\"  # Initialize an empty string to collect the limitation text\n",
        "\n",
        "      stream = client.chat.completions.create(\n",
        "          model=\"gpt-3.5-turbo-1106\", # gpt-3.5-turbo\n",
        "          # model=\"gpt-4o-mini\",\n",
        "          messages=[\n",
        "              {\n",
        "                  \"role\": \"user\",\n",
        "                  \"content\": prompt,\n",
        "              }\n",
        "          ],\n",
        "          stream=True,\n",
        "          temperature=0.2  # Adjust the temperature as needed, max_tokens=150\n",
        "      )\n",
        "\n",
        "      for chunk in stream:\n",
        "          summary_chunk = chunk.choices[0].delta.content or \"\"\n",
        "          summary_text += summary_chunk  # Append each chunk to the limitation_text\n",
        "\n",
        "      summary_chunks = []\n",
        "      summary_chunks.append(summary_text)\n",
        "      generated_limiations.append(summary_chunks)  # Append the collected limitation text to the list"
      ],
      "metadata": {
        "id": "nE1VpAb3EoI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_generated_future_work = pd.DataFrame(generated_limiations, columns=['Future Work'])\n",
        "df['Future_Work'] = df_generated_future_work['Future Work']"
      ],
      "metadata": {
        "id": "-wraD-bxeO0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install bert-score"
      ],
      "metadata": {
        "id": "dRJbaD-DeO0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "j0rQLtIYFCjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bert_score import BERTScorer\n",
        "\n",
        "refs = df['GPT_ground_truth_FW'].apply(lambda x: [str(x).strip()]).tolist()\n",
        "cands = df['Future_Work'].apply(lambda x: [str(x).strip()]).tolist()\n",
        "\n",
        "scorer = BERTScorer(model_type='roberta-large')\n",
        "\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "total_precision, total_recall, total_f1 = 0, 0, 0\n",
        "\n",
        "# Loop through each reference and candidate\n",
        "for ref, cand in zip(refs, cands):\n",
        "    if ref and cand:  # Ensure neither is empty\n",
        "        # Calculate scores\n",
        "        P, R, F1 = scorer.score(cand, ref)\n",
        "\n",
        "        # Convert to scalar values for printing and storing\n",
        "        precision = P.mean().item()\n",
        "        recall = R.mean().item()\n",
        "        f1_score = F1.mean().item()\n",
        "\n",
        "        # Append scores to respective lists\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1_score)\n",
        "\n",
        "        # Update total accumulators\n",
        "        total_precision += precision\n",
        "        total_recall += recall\n",
        "        total_f1 += f1_score\n",
        "\n",
        "        # Print each score along with refs and cands\n",
        "        print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1_score}\")\n",
        "        print(f\"Reference: {ref}\")\n",
        "        print(f\"Candidate: {cand}\\n\")\n",
        "\n",
        "# Calculate averages\n",
        "num_scores = len(precision_scores)\n",
        "average_precision = total_precision / num_scores if num_scores > 0 else 0\n",
        "average_recall = total_recall / num_scores if num_scores > 0 else 0\n",
        "average_f1 = total_f1 / num_scores if num_scores > 0 else 0\n",
        "\n",
        "# Add the individual scores to the DataFrame\n",
        "df['BERTScore_Precision'] = precision_scores\n",
        "df['BERTScore_Recall'] = recall_scores\n",
        "df['BERTScore_F1'] = f1_scores\n"
      ],
      "metadata": {
        "id": "aDqgXPSFFNEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install evaluate\n",
        "!pip -q install rouge_score"
      ],
      "metadata": {
        "id": "EbG4QKUveO0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.astype(str)\n",
        "\n",
        "def lcs(X, Y):\n",
        "    m = len(X)\n",
        "    n = len(Y)\n",
        "    L = [[0] * (n + 1) for i in range(m + 1)]\n",
        "\n",
        "    for i in range(m + 1):\n",
        "        for j in range(n + 1):\n",
        "            if i == 0 or j == 0:\n",
        "                L[i][j] = 0\n",
        "            elif X[i - 1] == Y[j - 1]:\n",
        "                L[i][j] = L[i - 1][j - 1] + 1\n",
        "            else:\n",
        "                L[i][j] = max(L[i - 1][j], L[i][j - 1])\n",
        "    return L[m][n]\n",
        "\n",
        "def rough_1(generated_summary, reference_summary):\n",
        "    gen_unigrams = set(generated_summary.split())\n",
        "    ref_unigrams = set(reference_summary.split())\n",
        "\n",
        "    common_unigrams = gen_unigrams.intersection(ref_unigrams)\n",
        "    precision = len(common_unigrams) / len(gen_unigrams)\n",
        "    return precision\n",
        "\n",
        "def rough_2(generated_summary, reference_summary):\n",
        "    gen_bigrams = set(zip(generated_summary.split(), generated_summary.split()[1:]))\n",
        "    ref_bigrams = set(zip(reference_summary.split(), reference_summary.split()[1:]))\n",
        "    common_bigrams = gen_bigrams.intersection(ref_bigrams)\n",
        "    precision = len(common_bigrams) / len(gen_bigrams)\n",
        "    return precision\n",
        "\n",
        "total_rough2_score = 0\n",
        "for i in range(len(df)):\n",
        "  rough_2_score = rough_2(df['Future_Work'][i], df['GPT_ground_truth_FW'][i])\n",
        "  total_rough2_score += rough_2_score\n",
        "print(\"average_rough_2_score\",total_rough2_score/len(df))\n",
        "\n",
        "\n",
        "total_rough1_score = 0\n",
        "for i in range(len(df)):\n",
        "  rough_1_score = rough_1(df['Future_Work'][i], df['GPT_ground_truth_FW'][i])\n",
        "  total_rough1_score += rough_1_score\n",
        "print(\"average_rough_1_score\",total_rough1_score/len(df))\n",
        "\n",
        "# cosine similarity\n",
        "def rough_l(generated_summary, reference_summary):\n",
        "    lcs_length = lcs(generated_summary.split(), reference_summary.split())\n",
        "    precision = lcs_length / len(reference_summary.split())\n",
        "    recall = lcs_length / len(generated_summary.split())\n",
        "\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1_score\n",
        "\n",
        "\n",
        "total_rough_l_score = 0\n",
        "for i in range(len(df)):\n",
        "  rough_l_score = rough_l(df['Future_Work'][i], df['GPT_ground_truth_FW'][i])\n",
        "  total_rough_l_score += rough_l_score\n",
        "print(\"average_rough_l_score\",total_rough_l_score/len(df))\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_similarity(text1, text2):\n",
        "    # Use CountVectorizer to convert text to a matrix of token counts\n",
        "    vectorizer = CountVectorizer().fit_transform([text1, text2])\n",
        "\n",
        "    # Calculate cosine similarity between the two texts\n",
        "    similarity_matrix = cosine_similarity(vectorizer)\n",
        "\n",
        "    # The similarity_matrix[0, 1] contains the cosine similarity between text1 and text2\n",
        "    return similarity_matrix[0, 1]\n",
        "\n",
        "def calculate_average_rough_score(text1, text2):\n",
        "    # Calculate similarity scores\n",
        "    similarity_score1 = calculate_similarity(text1, text2)\n",
        "    similarity_score2 = calculate_similarity(text2, text1)\n",
        "\n",
        "    # Calculate average rough score\n",
        "    average_rough_score = (similarity_score1 + similarity_score2) / 2\n",
        "\n",
        "    return average_rough_score\n",
        "\n",
        "total_score = 0  # Initialize a variable to accumulate scores\n",
        "\n",
        "for i in range(len(df)):\n",
        "    # reference_summary = df_grouped_sentence['Sentence'][i] test_data['generated_future_work'][i], test_data['Extracted Future Work'][i]\n",
        "    reference_summary = df['GPT_ground_truth_FW'][i]\n",
        "    generated_summary = df['Future_Work'][i]\n",
        "    average_rough_score = calculate_average_rough_score(reference_summary, generated_summary)\n",
        "\n",
        "    total_score += average_rough_score\n",
        "\n",
        "# Calculate the overall average\n",
        "overall_average = total_score / len(df)\n",
        "\n",
        "# Print the overall average\n",
        "print(f\"\\nOverall Average cosine similarity Score: {overall_average}\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def jaccard_similarity(text1, text2):\n",
        "    # Tokenize the texts by splitting on whitespace\n",
        "    set1 = set(text1.split())\n",
        "    set2 = set(text2.split())\n",
        "\n",
        "    # Calculate the intersection and union of the sets\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "\n",
        "    # Compute Jaccard similarity\n",
        "    return len(intersection) / len(union) if union else 0\n",
        "\n",
        "# Calculate Jaccard similarity for each pair of rows test_data['Extracted Future Work'][i]\n",
        "jaccard_similarities = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    similarity = jaccard_similarity(df.loc[i, 'GPT_ground_truth_FW'], df.loc[i, 'Future_Work'])\n",
        "    jaccard_similarities.append(similarity)\n",
        "\n",
        "# Add the Jaccard similarity as a new column in df1\n",
        "df['Jaccard_Similarity'] = jaccard_similarities\n",
        "\n",
        "# Calculate the average Jaccard similarity\n",
        "average_jaccard_similarity = sum(jaccard_similarities) / len(jaccard_similarities)\n",
        "\n",
        "print(\"average jaccard similarity\",average_jaccard_similarity)\n",
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "def calculate_bleu(reference, candidate):\n",
        "    # Tokenize the sentences using nltk's word_tokenize function\n",
        "    reference_tokens = word_tokenize(reference.lower())\n",
        "    candidate_tokens = word_tokenize(candidate.lower())\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    score = sentence_bleu([reference_tokens], candidate_tokens)\n",
        "    return score\n",
        "\n",
        "# First, ensure that both columns are converted to string type and handle any missing values\n",
        "df['GPT_ground_truth_FW'] = df['GPT_ground_truth_FW'].astype(str)\n",
        "df['Future_Work'] = df['Future_Work'].astype(str)\n",
        "\n",
        "# Calculate BLEU scores using a list comprehension and store in a list\n",
        "bleu_scores = [calculate_bleu(ref, cand) for ref, cand in zip(df['GPT_ground_truth_FW'],df['Future_Work'])]\n",
        "\n",
        "# Optional: Store the BLEU scores back into a DataFrame for analysis\n",
        "results_df = pd.DataFrame({'BLEU Score': bleu_scores})\n",
        "\n",
        "# Calculate the average BLEU score\n",
        "average_bleu_score = results_df['BLEU Score'].mean()\n",
        "\n",
        "print(f\"Average BLEU Score: {average_bleu_score}\")"
      ],
      "metadata": {
        "id": "2UsAJv8eFXgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5de4f87-3b01-4f3c-e829-84dbca7734aa",
        "id": "JRrEgL9deO0r"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/106.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluation by LLM"
      ],
      "metadata": {
        "id": "P9GZOIQpHGu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "prompt1 = \"\"\"Title: Evaluation of Text Quality\n",
        "\n",
        "Instructions:\n",
        "\n",
        "You are provided with two texts for each pair: one is generated by a machine (Machine-Generated Text), and the other is the original or ground truth text (Ground Truth). Please read both texts carefully. After reviewing each text, assign a score from 1 to 5 based on the criteria outlined below. The score should reflect how well the machine-generated text compares to the ground truth, where 1 represents poor quality and 5 represents excellent quality that closely matches or even surpasses the ground truth in some aspects.\n",
        "\n",
        "Scoring Criteria:\n",
        "Coherence and Logic:\n",
        "\n",
        "5: The text is exceptionally coherent; the ideas flow logically and are well connected.\n",
        "3: The text is coherent but may have occasional lapses in logic or flow.\n",
        "1: The text is disjointed or frequently illogical.\n",
        "Relevance and Accuracy:\n",
        "\n",
        "5: The text is completely relevant to the topic and accurate in all presented facts.\n",
        "3: The text is generally relevant with minor factual errors or slight deviations from the topic.\n",
        "1: The text often strays off topic or includes multiple factual inaccuracies.\n",
        "Readability and Style:\n",
        "\n",
        "5: The text is engaging, well-written, and stylistically consistent with the ground truth.\n",
        "3: The text is readable but may lack flair or have minor stylistic inconsistencies.\n",
        "1: The text is difficult to read or stylistically poor.\n",
        "Grammatical Correctness:\n",
        "\n",
        "5: The text is free from grammatical errors.\n",
        "3: The text has occasional grammatical errors that do not impede understanding.\n",
        "1: The text has frequent grammatical errors that hinder comprehension.\n",
        "Overall Impression:\n",
        "\n",
        "5: The text is of a quality that you would expect from a professional writer.\n",
        "3: The text is acceptable but would benefit from further editing.\n",
        "1: The text is of a quality that needs significant revision to be usable.\n",
        "Task:\n",
        "\n",
        "For each text pair:\n",
        "\n",
        "Rate the Machine-Generated Text on each criterion and provide a final overall score out of 5.\n",
        "Provide a brief justification for your scores, highlighting strengths and weaknesses observed in the machine-generated text relative to the ground truth.\n",
        "Example of Usage:\n",
        "Text Pair 1:\n",
        "\n",
        "Machine-Generated Text: \"The quick brown fox jumps over the lazy dog repeatedly.\"\n",
        "\n",
        "Ground Truth: \"A quick brown fox consistently jumps over the lazy dog.\"\n",
        "\n",
        "Evaluation:\n",
        "\n",
        "Coherence and Logic: 5\n",
        "Relevance and Accuracy: 4\n",
        "Readability and Style: 5\n",
        "Grammatical Correctness: 5\n",
        "Overall Impression: 5\n",
        "Justification: The machine-generated text maintains the core message and style of the ground truth, presenting it coherently and engagingly. Minor variations in wording do not impact the overall quality or relevance of the message. \"\"\""
      ],
      "metadata": {
        "id": "tUlCSErneO0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(\n",
        "    api_key=userdata.get('GROQ_API_KEY'),\n",
        ")"
      ],
      "metadata": {
        "id": "YOUdewNveO0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "LLM_feedback = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    prompt2 = f\"Ground Truth: {df['GPT_ground_truth_FW'][i]} Machine Generate Text: {df['Future_Work'][i]}\"\n",
        "    prompt = prompt1 + prompt2\n",
        "    summary_text = \"\"  # Initialize an empty string to collect the limitation text\n",
        "\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        # model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0.2\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        feedback_chunk = chunk.choices[0].delta.content or \"\"\n",
        "        feedback_text += feedback_chunk\n",
        "\n",
        "    feedback_chunks = []\n",
        "    feedback_chunks.append(feedback_text)\n",
        "    LLM_feedback.append(feedback_chunks)"
      ],
      "metadata": {
        "id": "GZandm_9GRk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_LLM_feedback = pd.DataFrame(generated_summary, columns=['LLM_feedback'])"
      ],
      "metadata": {
        "id": "RZD7PAE-eO0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Function to extract scores from each category\n",
        "def extract_score(text, category):\n",
        "    pattern = rf\"{category}: (\\d)\"\n",
        "    score = re.search(pattern, text)\n",
        "    return int(score.group(1)) if score else np.nan\n",
        "\n",
        "# Apply the function for each category\n",
        "df_LLM_feedback['Coherence and Logic'] = df_LLM_feedback['LLM_feedback'].apply(extract_score, category='Coherence and Logic')\n",
        "df_LLM_feedback['Relevance and Accuracy'] = df_LLM_feedback['LLM_feedback'].apply(extract_score, category='Relevance and Accuracy')\n",
        "df_LLM_feedback['Readability and Style'] = df_LLM_feedback['LLM_feedback'].apply(extract_score, category='Readability and Style')\n",
        "df_LLM_feedback['Grammatical Correctness'] = df_LLM_feedback['LLM_feedback'].apply(extract_score, category='Grammatical Correctness')\n",
        "df_LLM_feedback['Overall Impression'] = df_LLM_feedback['LLM_feedback'].apply(extract_score, category='Overall Impression')\n",
        "\n",
        "# Calculate average for each category\n",
        "average_coherence_logic = df_LLM_feedback['Coherence and Logic'].mean()\n",
        "average_relevance_accuracy = df_LLM_feedback['Relevance and Accuracy'].mean()\n",
        "average_readability_style = df_LLM_feedback['Readability and Style'].mean()\n",
        "average_grammatical_correctness = df_LLM_feedback['Grammatical Correctness'].mean()\n",
        "average_overall_impression = df_LLM_feedback['Overall Impression'].mean()"
      ],
      "metadata": {
        "id": "8Qzr8Wk1GXJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# take the feedback who doesn't pass the threshold and send to LLM (3rd iteration)"
      ],
      "metadata": {
        "id": "1OsMD630yvlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['BERTScore_F1'] = pd.to_numeric(df['BERTScore_F1'], errors='coerce')\n",
        "df['Coherence and Logic'] = pd.to_numeric(df['Coherence and Logic'], errors='coerce')\n",
        "df['Relevance and Accuracy'] = pd.to_numeric(df['Relevance and Accuracy'], errors='coerce')\n",
        "df['Readability and Style'] = pd.to_numeric(df['Readability and Style'], errors='coerce')\n",
        "df['Grammatical Correctness'] = pd.to_numeric(df['Grammatical Correctness'], errors='coerce')\n",
        "df['Overall Impression'] = pd.to_numeric(df['Overall Impression'], errors='coerce')\n"
      ],
      "metadata": {
        "id": "yy0Hl1Cr0172"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1a6e07-c1ea-4830-95fa-fa2f80bed74b",
        "id": "HpQcRkKyyvlg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/386.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m358.4/386.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean up the text\n",
        "def clean_and_extract_justification(text):\n",
        "    # Remove all newline characters\n",
        "    text = text.replace('\\n', ' ')\n",
        "    # Find the start of \"Justification\" and remove everything before it\n",
        "    justification_start = text.find(\"Justification\")\n",
        "    if justification_start != -1:\n",
        "        return text[justification_start:]  # Keep text from \"Justification\" onward\n",
        "    else:\n",
        "        return text  # Return the original text if \"Justification\" is not found\n",
        "\n",
        "# Apply the function to the 'LLM_feedback' column\n",
        "df['Refined_LLM_feedback'] = df['LLM_feedback'].apply(clean_and_extract_justification)\n"
      ],
      "metadata": {
        "id": "tA9ThYilyvlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now use the model to send the problem you found and generate 'refined' limitaions\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "generated_limiations = []\n",
        "# abstracts = []\n",
        "# Abstract\tIntroduction conclusion\tRelated work\tmethodology\t\texperiment and results\textra\n",
        "for i in range(len(df)):\n",
        "    coherence_score = df['Coherence and Logic'][i]\n",
        "    relevance_score = df['Relevance and Accuracy'][i]\n",
        "    readability_score = df['Readability and Style'][i]\n",
        "    grammar_score = df['Grammatical Correctness'][i]\n",
        "    overall_score = df['Overall Impression'][i]\n",
        "    bscore_f1 = df['BERTScore_F1'][i]\n",
        "\n",
        "    if coherence_score > 3 and relevance_score > 3 and readability_score > 3 and grammar_score > 3 and overall_score > 3 and bscore_f1 > 0.86:\n",
        "      generated_limiations.append([df['Future_Work'][i]])\n",
        "\n",
        "    else:\n",
        "      print(\"else\",i)\n",
        "      prompt_template = \"\"\"You are an AI model trained to analyze scientific content and provide high-quality future research suggestions. Below, you will find sections\n",
        "      from a scientific article, along with a justification for improving the \"Future Work\" section. Please use the article content and justification to generate an enhanced\n",
        "      \"Future Work\" section that addresses the limitations mentioned.\n",
        "\n",
        "      Your goal is to create a cohesive, logically structured, and engaging \"Future Work\" section that aligns closely with the article's content.\n",
        "      The suggestion should also consider areas of improvement outlined in the justification, such as Coherence and Logic, Relevance and Accuracy, Readability and Style,\n",
        "      Grammatical Correctness, and precision. Aim for a concise response within 100 words.\n",
        "\n",
        "      Article Content:\n",
        "      Abstract:\n",
        "      \"{abstract_text}\"\n",
        "\n",
        "      Introduction:\n",
        "      \"{introduction_text}\"\n",
        "\n",
        "      Conclusion:\n",
        "      \"{conclusion_text}\"\n",
        "\n",
        "      Justification:\n",
        "      \"{justification_text}\"\n",
        "\n",
        "      Based on the article content and the justification provided, generate an improved \"Future Work\" section that builds on the strengths of the original while addressing\n",
        "      the noted weaknesses. Ensure your response is aligned with the main topic and presents clear directions for future research.\n",
        "      \"\"\"\n",
        "\n",
        "      # Replace placeholders with the actual content from the DataFrame\n",
        "      abstract_text = df.loc[i, 'Abstract']  # Replace 0 with the actual row index if needed\n",
        "      introduction_text = df.loc[i, 'Introduction']\n",
        "      conclusion_text = df.loc[i, 'Conclusion']\n",
        "      justification_text = df.loc[i, 'Refined_LLM_feedback']\n",
        "\n",
        "      # Format the final prompt\n",
        "      prompt = prompt_template.format(\n",
        "          abstract_text=abstract_text,\n",
        "          introduction_text=introduction_text,\n",
        "          conclusion_text=conclusion_text,\n",
        "          justification_text=justification_text\n",
        "      )\n",
        "\n",
        "      summary_text = \"\"  # Initialize an empty string to collect the limitation text\n",
        "\n",
        "      stream = client.chat.completions.create(\n",
        "          model=\"gpt-3.5-turbo-1106\", # gpt-3.5-turbo\n",
        "          # model=\"gpt-4o-mini\",\n",
        "          messages=[\n",
        "              {\n",
        "                  \"role\": \"user\",\n",
        "                  \"content\": prompt,\n",
        "              }\n",
        "          ],\n",
        "          stream=True,\n",
        "          temperature=0.2  # Adjust the temperature as needed, max_tokens=150\n",
        "      )\n",
        "\n",
        "      for chunk in stream:\n",
        "          summary_chunk = chunk.choices[0].delta.content or \"\"\n",
        "          summary_text += summary_chunk  # Append each chunk to the limitation_text\n",
        "\n",
        "      summary_chunks = []\n",
        "      summary_chunks.append(summary_text)\n",
        "      generated_limiations.append(summary_chunks)  # Append the collected limitation text to the list"
      ],
      "metadata": {
        "id": "u3UhVvudKjyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_generated_future_work = pd.DataFrame(generated_limiations, columns=['Future Work'])\n",
        "df['Future_Work'] = df_generated_future_work['Future Work']"
      ],
      "metadata": {
        "id": "PG4xb82gyvli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "3vzogKbRMiqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install bert-score"
      ],
      "metadata": {
        "id": "DdNJaSK2yvli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bert_score import BERTScorer\n",
        "\n",
        "# Convert each 'Limitation' row to a list containing that single row's string\n",
        "refs = df['GPT_ground_truth_FW'][:39].apply(lambda x: [str(x).strip()]).tolist()\n",
        "cands = df['Future_Work'][:39].apply(lambda x: [str(x).strip()]).tolist()\n",
        "\n",
        "# Initialize the scorer\n",
        "scorer = BERTScorer(model_type='roberta-large')\n",
        "\n",
        "# Initialize lists to store individual scores and accumulators for averages\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "total_precision, total_recall, total_f1 = 0, 0, 0\n",
        "\n",
        "# Loop through each reference and candidate\n",
        "for ref, cand in zip(refs, cands):\n",
        "    if ref and cand:  # Ensure neither is empty\n",
        "        # Calculate scores\n",
        "        P, R, F1 = scorer.score(cand, ref)\n",
        "\n",
        "        # Convert to scalar values for printing and storing\n",
        "        precision = P.mean().item()\n",
        "        recall = R.mean().item()\n",
        "        f1_score = F1.mean().item()\n",
        "\n",
        "        # Append scores to respective lists\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1_score)\n",
        "\n",
        "        # Update total accumulators\n",
        "        total_precision += precision\n",
        "        total_recall += recall\n",
        "        total_f1 += f1_score\n",
        "\n",
        "        # Print each score along with refs and cands\n",
        "        print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1_score}\")\n",
        "        print(f\"Reference: {ref}\")\n",
        "        print(f\"Candidate: {cand}\\n\")\n",
        "\n",
        "# Calculate averages\n",
        "num_scores = len(precision_scores)\n",
        "average_precision = total_precision / num_scores if num_scores > 0 else 0\n",
        "average_recall = total_recall / num_scores if num_scores > 0 else 0\n",
        "average_f1 = total_f1 / num_scores if num_scores > 0 else 0\n",
        "\n",
        "# Print average scores\n",
        "print(f\"Average Precision: {average_precision:.4f}\")\n",
        "print(f\"Average Recall: {average_recall:.4f}\")\n",
        "print(f\"Average F1 Score: {average_f1:.4f}\")\n",
        "\n",
        "# Add the individual scores to the DataFrame\n",
        "df['BERTScore_Precision'][:39] = precision_scores\n",
        "df['BERTScore_Recall'][:39] = recall_scores\n",
        "df['BERTScore_F1'][:39] = f1_scores\n"
      ],
      "metadata": {
        "id": "4-2sC3bRMs5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install evaluate\n",
        "!pip -q install rouge_score"
      ],
      "metadata": {
        "id": "Pc7f8Tx1yvlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.astype(str)\n",
        "\n",
        "def lcs(X, Y):\n",
        "    m = len(X)\n",
        "    n = len(Y)\n",
        "    L = [[0] * (n + 1) for i in range(m + 1)]\n",
        "\n",
        "    for i in range(m + 1):\n",
        "        for j in range(n + 1):\n",
        "            if i == 0 or j == 0:\n",
        "                L[i][j] = 0\n",
        "            elif X[i - 1] == Y[j - 1]:\n",
        "                L[i][j] = L[i - 1][j - 1] + 1\n",
        "            else:\n",
        "                L[i][j] = max(L[i - 1][j], L[i][j - 1])\n",
        "    return L[m][n]\n",
        "\n",
        "def rough_1(generated_summary, reference_summary):\n",
        "    gen_unigrams = set(generated_summary.split())\n",
        "    ref_unigrams = set(reference_summary.split())\n",
        "\n",
        "    common_unigrams = gen_unigrams.intersection(ref_unigrams)\n",
        "    precision = len(common_unigrams) / len(gen_unigrams)\n",
        "    return precision\n",
        "\n",
        "def rough_2(generated_summary, reference_summary):\n",
        "    gen_bigrams = set(zip(generated_summary.split(), generated_summary.split()[1:]))\n",
        "    ref_bigrams = set(zip(reference_summary.split(), reference_summary.split()[1:]))\n",
        "    common_bigrams = gen_bigrams.intersection(ref_bigrams)\n",
        "    precision = len(common_bigrams) / len(gen_bigrams)\n",
        "    return precision\n",
        "\n",
        "total_rough2_score = 0\n",
        "for i in range(len(df)):\n",
        "  rough_2_score = rough_2(df['Future_Work'][i], df['GPT_ground_truth_FW'][i])\n",
        "  total_rough2_score += rough_2_score\n",
        "print(\"average_rough_2_score\",total_rough2_score/len(df))\n",
        "\n",
        "\n",
        "total_rough1_score = 0\n",
        "for i in range(len(df)):\n",
        "  rough_1_score = rough_1(df['Future_Work'][i], df['GPT_ground_truth_FW'][i])\n",
        "  total_rough1_score += rough_1_score\n",
        "print(\"average_rough_1_score\",total_rough1_score/len(df))\n",
        "\n",
        "# cosine similarity\n",
        "def rough_l(generated_summary, reference_summary):\n",
        "    lcs_length = lcs(generated_summary.split(), reference_summary.split())\n",
        "    precision = lcs_length / len(reference_summary.split())\n",
        "    recall = lcs_length / len(generated_summary.split())\n",
        "\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1_score\n",
        "\n",
        "total_rough_l_score = 0\n",
        "for i in range(len(df)):\n",
        "  rough_l_score = rough_l(df['Future_Work'][i], df['GPT_ground_truth_FW'][i])\n",
        "  total_rough_l_score += rough_l_score\n",
        "print(\"average_rough_l_score\",total_rough_l_score/len(df))\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_similarity(text1, text2):\n",
        "    # Use CountVectorizer to convert text to a matrix of token counts\n",
        "    vectorizer = CountVectorizer().fit_transform([text1, text2])\n",
        "\n",
        "    # Calculate cosine similarity between the two texts\n",
        "    similarity_matrix = cosine_similarity(vectorizer)\n",
        "\n",
        "    # The similarity_matrix[0, 1] contains the cosine similarity between text1 and text2\n",
        "    return similarity_matrix[0, 1]\n",
        "\n",
        "def calculate_average_rough_score(text1, text2):\n",
        "    # Calculate similarity scores\n",
        "    similarity_score1 = calculate_similarity(text1, text2)\n",
        "    similarity_score2 = calculate_similarity(text2, text1)\n",
        "\n",
        "    # Calculate average rough score\n",
        "    average_rough_score = (similarity_score1 + similarity_score2) / 2\n",
        "\n",
        "    return average_rough_score\n",
        "\n",
        "total_score = 0  # Initialize a variable to accumulate scores\n",
        "\n",
        "for i in range(len(df)):\n",
        "    # reference_summary = df_grouped_sentence['Sentence'][i] test_data['generated_future_work'][i], test_data['Extracted Future Work'][i]\n",
        "    reference_summary = df['GPT_ground_truth_FW'][i]\n",
        "    generated_summary = df['Future_Work'][i]\n",
        "    average_rough_score = calculate_average_rough_score(reference_summary, generated_summary)\n",
        "\n",
        "    # print(f\"Similarity Score of Topic {i + 1}: {average_rough_score}\")\n",
        "\n",
        "    # Add the current score to the total\n",
        "    total_score += average_rough_score\n",
        "\n",
        "# Calculate the overall average\n",
        "overall_average = total_score / len(df)\n",
        "\n",
        "# Print the overall average\n",
        "print(f\"\\nOverall Average cosine similarity Score: {overall_average}\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def jaccard_similarity(text1, text2):\n",
        "    # Tokenize the texts by splitting on whitespace\n",
        "    set1 = set(text1.split())\n",
        "    set2 = set(text2.split())\n",
        "\n",
        "    # Calculate the intersection and union of the sets\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "\n",
        "    # Compute Jaccard similarity\n",
        "    return len(intersection) / len(union) if union else 0\n",
        "\n",
        "# Calculate Jaccard similarity for each pair of rows test_data['Extracted Future Work'][i]\n",
        "jaccard_similarities = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    similarity = jaccard_similarity(df.loc[i, 'GPT_ground_truth_FW'], df.loc[i, 'Future_Work'])\n",
        "    jaccard_similarities.append(similarity)\n",
        "\n",
        "# Add the Jaccard similarity as a new column in df1\n",
        "df['Jaccard_Similarity'] = jaccard_similarities\n",
        "\n",
        "# Calculate the average Jaccard similarity\n",
        "average_jaccard_similarity = sum(jaccard_similarities) / len(jaccard_similarities)\n",
        "\n",
        "print(\"average jaccard similarity\",average_jaccard_similarity)\n",
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "def calculate_bleu(reference, candidate):\n",
        "    # Tokenize the sentences using nltk's word_tokenize function\n",
        "    reference_tokens = word_tokenize(reference.lower())\n",
        "    candidate_tokens = word_tokenize(candidate.lower())\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    score = sentence_bleu([reference_tokens], candidate_tokens)\n",
        "    return score\n",
        "\n",
        "# First, ensure that both columns are converted to string type and handle any missing values\n",
        "df['GPT_ground_truth_FW'] = df['GPT_ground_truth_FW'].astype(str)\n",
        "df['Future_Work'] = df['Future_Work'].astype(str)\n",
        "\n",
        "# Calculate BLEU scores using a list comprehension and store in a list\n",
        "bleu_scores = [calculate_bleu(ref, cand) for ref, cand in zip(df['GPT_ground_truth_FW'],df['Future_Work'])]\n",
        "\n",
        "# Optional: Store the BLEU scores back into a DataFrame for analysis\n",
        "results_df = pd.DataFrame({'BLEU Score': bleu_scores})\n",
        "\n",
        "# Calculate the average BLEU score\n",
        "average_bleu_score = results_df['BLEU Score'].mean()\n",
        "\n",
        "print(f\"Average BLEU Score: {average_bleu_score}\")"
      ],
      "metadata": {
        "id": "E47PyxOiMv24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5de4f87-3b01-4f3c-e829-84dbca7734aa",
        "id": "wSL1q0EOyvlj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/106.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluating by LLM"
      ],
      "metadata": {
        "id": "Ic88MmCWNEva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "prompt1 = \"\"\"Title: Evaluation of Text Quality\n",
        "\n",
        "Instructions:\n",
        "\n",
        "You are provided with two texts for each pair: one is generated by a machine (Machine-Generated Text), and the other is the original or ground truth text (Ground Truth). Please read both texts carefully. After reviewing each text, assign a score from 1 to 5 based on the criteria outlined below. The score should reflect how well the machine-generated text compares to the ground truth, where 1 represents poor quality and 5 represents excellent quality that closely matches or even surpasses the ground truth in some aspects.\n",
        "\n",
        "Scoring Criteria:\n",
        "Coherence and Logic:\n",
        "\n",
        "5: The text is exceptionally coherent; the ideas flow logically and are well connected.\n",
        "3: The text is coherent but may have occasional lapses in logic or flow.\n",
        "1: The text is disjointed or frequently illogical.\n",
        "Relevance and Accuracy:\n",
        "\n",
        "5: The text is completely relevant to the topic and accurate in all presented facts.\n",
        "3: The text is generally relevant with minor factual errors or slight deviations from the topic.\n",
        "1: The text often strays off topic or includes multiple factual inaccuracies.\n",
        "Readability and Style:\n",
        "\n",
        "5: The text is engaging, well-written, and stylistically consistent with the ground truth.\n",
        "3: The text is readable but may lack flair or have minor stylistic inconsistencies.\n",
        "1: The text is difficult to read or stylistically poor.\n",
        "Grammatical Correctness:\n",
        "\n",
        "5: The text is free from grammatical errors.\n",
        "3: The text has occasional grammatical errors that do not impede understanding.\n",
        "1: The text has frequent grammatical errors that hinder comprehension.\n",
        "Overall Impression:\n",
        "\n",
        "5: The text is of a quality that you would expect from a professional writer.\n",
        "3: The text is acceptable but would benefit from further editing.\n",
        "1: The text is of a quality that needs significant revision to be usable.\n",
        "Task:\n",
        "\n",
        "For each text pair:\n",
        "\n",
        "Rate the Machine-Generated Text on each criterion and provide a final overall score out of 5.\n",
        "Provide a brief justification for your scores, highlighting strengths and weaknesses observed in the machine-generated text relative to the ground truth.\n",
        "Example of Usage:\n",
        "Text Pair 1:\n",
        "\n",
        "Machine-Generated Text: \"The quick brown fox jumps over the lazy dog repeatedly.\"\n",
        "\n",
        "Ground Truth: \"A quick brown fox consistently jumps over the lazy dog.\"\n",
        "\n",
        "Evaluation:\n",
        "\n",
        "Coherence and Logic: 5\n",
        "Relevance and Accuracy: 4\n",
        "Readability and Style: 5\n",
        "Grammatical Correctness: 5\n",
        "Overall Impression: 5\n",
        "Justification: The machine-generated text maintains the core message and style of the ground truth, presenting it coherently and engagingly. Minor variations in wording do not impact the overall quality or relevance of the message. \"\"\""
      ],
      "metadata": {
        "id": "NdVCtDYRyvlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(\n",
        "    api_key=userdata.get('GROQ_API_KEY'),\n",
        ")"
      ],
      "metadata": {
        "id": "_fe8iBZIyvlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "LLM_feedback = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    prompt2 = f\"Ground Truth: {df['GPT_ground_truth_FW'][i]} Machine Generate Text: {df['Future_Work'][i]}\"\n",
        "    prompt = prompt1 + prompt2\n",
        "    summary_text = \"\"  # Initialize an empty string to collect the limitation text\n",
        "\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        # model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0.2\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        feedback_chunk = chunk.choices[0].delta.content or \"\"\n",
        "        feedback_text += feedback_chunk\n",
        "\n",
        "    feedback_chunks = []\n",
        "    feedback_chunks.append(feedback_text)\n",
        "    LLM_feedback.append(feedback_chunks)"
      ],
      "metadata": {
        "id": "GsATC013LeUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_LLM_feedback = pd.DataFrame(generated_summary, columns=['LLM_feedback'])\n"
      ],
      "metadata": {
        "id": "MTbKR9_Ryvlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Function to extract scores from each category\n",
        "def extract_score(text, category):\n",
        "    pattern = rf\"{category}: (\\d)\"\n",
        "    score = re.search(pattern, text)\n",
        "    return int(score.group(1)) if score else np.nan\n",
        "\n",
        "# Apply the function for each category\n",
        "df_LLM_feedback['Coherence and Logic'] = df_LLM_feedback['LLM_feedback'].apply(extract_score, category='Coherence and Logic')\n",
        "df_LLM_feedback['Relevance and Accuracy'] = df_LLM_feedback['LLM_feedback'].apply(extract_score, category='Relevance and Accuracy')\n",
        "df_LLM_feedback['Readability and Style'] = df_LLM_feedback['LLM_feedback'].apply(extract_score, category='Readability and Style')\n",
        "df_LLM_feedback['Grammatical Correctness'] = df_LLM_feedback['LLM_feedback'].apply(extract_score, category='Grammatical Correctness')\n",
        "df_LLM_feedback['Overall Impression'] = df_LLM_feedback['LLM_feedback'].apply(extract_score, category='Overall Impression')\n",
        "\n",
        "# Calculate average for each category\n",
        "average_coherence_logic = df_LLM_feedback['Coherence and Logic'].mean()\n",
        "average_relevance_accuracy = df_LLM_feedback['Relevance and Accuracy'].mean()\n",
        "average_readability_style = df_LLM_feedback['Readability and Style'].mean()\n",
        "average_grammatical_correctness = df_LLM_feedback['Grammatical Correctness'].mean()\n",
        "average_overall_impression = df_LLM_feedback['Overall Impression'].mean()"
      ],
      "metadata": {
        "id": "EpMbxYsnLgrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('dataset/df_LLM_feedback.csv', index=False)  # Set index=False if you don't want to save the row indices"
      ],
      "metadata": {
        "id": "ep8M8fmj3QOs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}