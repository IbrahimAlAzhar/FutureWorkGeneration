{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Performance measurement"
      ],
      "metadata": {
        "id": "3wRzzTmo1wUD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrBAxkdB1qeq"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "Instructions: You are provided with two texts for each pair: one is Author-Mentioned Future Work and another is\n",
        "LLM-Generated Future Work. Please read both texts carefully. After reviewing each text, assign a score from\n",
        "1 to 5 for each criterion outlined below. The score should reflect how well the LLM-Generated Future Work compares to the\n",
        "Author-Mentioned Future Work,\n",
        "where 1 represents poor quality and 5 represents excellent quality that closely matches or even surpasses the Author-Mentioned Future Work\n",
        "in some aspects.\n",
        "\n",
        "Author-Mentioned Future Work:\n",
        "<Author-Mentioned Future Work>\n",
        "\n",
        "LLM-Generated Future Work:\n",
        "<LLM-Generated Future Work>\n",
        "\n",
        "Scoring Criteria:\n",
        "Coherence and Logic:\n",
        "\n",
        "5: The text is exceptionally coherent; the ideas flow logically and are well connected.\n",
        "\n",
        "3: The text is coherent but may have occasional lapses in logic or flow.\n",
        "\n",
        "1: The text is disjointed or frequently illogical.\n",
        "\n",
        "Relevance and Accuracy:\n",
        "\n",
        "5: The text is completely relevant to the topic and accurate in all presented facts.\n",
        "\n",
        "3: The text is generally relevant with minor factual errors or slight deviations from the topic.\n",
        "\n",
        "1: The text often strays off topic or includes multiple factual inaccuracies.\n",
        "\n",
        "Readability and Style:\n",
        "\n",
        "5: The text is engaging, well-written, and stylistically consistent with the Author-Mentioned Future Work\n",
        "3: The text is readable but may lack flair or have minor stylistic inconsistencies.\n",
        "1: The text is difficult to read or stylistically poor.\n",
        "\n",
        "Grammatical Correctness:\n",
        "\n",
        "5: The text is free from grammatical errors.\n",
        "3: The text has occasional grammatical errors that do not impede understanding.\n",
        "1: The text has frequent grammatical errors that hinder comprehension.\n",
        "\n",
        "Overall Impression:\n",
        "5: The text is of a quality that you would expect from a professional writer.\n",
        "3: The text is acceptable but would benefit from further editing.\n",
        "1: The text is of a quality that needs significant revision to be usable.\n",
        "\n",
        "Task: For each text pair:\n",
        "Rate the LLM-Generated Future Work on each criterion and provide a final overall score out of 5.\n",
        "Provide a justification for each criterion score, highlighting strengths and weaknesses observed in the LLM-Generated Future Work\n",
        "relative to the Author-Mentioned Future Work.\n",
        "Present the scores and justifications in JSON format, structured as follows:\n",
        "\n",
        "{ \"Coherence and Logic\": { \"score\": , \"justification\": \"\" }, \"Relevance and Accuracy\": { \"score\": , \"justification\": \"\" },\n",
        "\"Readability and Style\": { \"score\": , \"justification\": \"\" }, \"Grammatical Correctness\": { \"score\": , \"justification\": \"\" },\n",
        "\"Overall Impression\": { \"score\": , \"justification\": \"\" } }\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "# Choose the encoding for your model:\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "MAX_CTX = 128_000  # model’s max context length in tokens\n",
        "\n",
        "def truncate_to_limit(text: str, max_tokens: int) -> str:\n",
        "    tokens = enc.encode(text)\n",
        "    if len(tokens) > max_tokens:\n",
        "        # slice to max_tokens and decode back to string\n",
        "        tokens = tokens[:max_tokens]\n",
        "        return enc.decode(tokens)\n",
        "    return text\n",
        "\n",
        "# … your existing setup …\n",
        "\n",
        "generated_summary = []\n",
        "\n",
        "for i in range(len(df_rest)): # len(df_rest)\n",
        "    # build the raw prompt body\n",
        "    # author_str    = \"\\n\".join(df_rest.at[i, 'LLM_extracted_future_work'])\n",
        "    # generated_str = \"\\n\".join(df_rest.at[i, 'RAG_generated_fw_from_paper'])\n",
        "\n",
        "    raw_body = (\n",
        "        prompt\n",
        "        + \"Author-Mentioned Future Work:\\n\" + df_rest['LLM_extracted_future_work'][i]\n",
        "        + \"\\n\\nLLM-Generated Future Work:\\n\" + df_rest['RAG_generated_fw_from_paper'][i]\n",
        "    )\n",
        "\n",
        "    # truncate if necessary\n",
        "    body = truncate_to_limit(raw_body, MAX_CTX)\n",
        "\n",
        "    # now send the request with the (possibly truncated) body\n",
        "    summary_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": body}],\n",
        "        stream=True,\n",
        "        temperature=0.1,\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        summary_text += chunk.choices[0].delta.content or \"\"\n",
        "\n",
        "    generated_summary.append([summary_text])\n"
      ],
      "metadata": {
        "id": "OuFDha6v1vj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, json\n",
        "import pandas as pd\n",
        "# import ace_tools as tools\n",
        "\n",
        "# 1) Flatten your list-of-lists\n",
        "flat_snippets = [item[0] for item in generated_summary]\n",
        "\n",
        "records = []\n",
        "for snippet in flat_snippets:\n",
        "    # 2) Isolate the first “{” through the last “}”\n",
        "    start = snippet.find('{')\n",
        "    end   = snippet.rfind('}')\n",
        "    if start == -1 or end == -1:\n",
        "        continue\n",
        "    json_str = snippet[start:end+1]\n",
        "\n",
        "    # 3) Remove any trailing commas before } or ]\n",
        "    json_str = re.sub(r',\\s*([}\\]])', r'\\1', json_str)\n",
        "\n",
        "    # 4) Load it\n",
        "    try:\n",
        "        data = json.loads(json_str)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Failed to parse snippet:\", e)\n",
        "        print(json_str)\n",
        "        continue\n",
        "\n",
        "    # 5) Flatten into a single dict\n",
        "    flat = {}\n",
        "    for metric, info in data.items():\n",
        "        key = metric.replace(\" \", \"_\")\n",
        "        flat[f\"{key}_score\"]         = info.get(\"score\")\n",
        "        flat[f\"{key}_justification\"] = info.get(\"justification\")\n",
        "    records.append(flat)\n",
        "\n",
        "# 6) Build your DataFrame\n",
        "df_gpt_judge21 = pd.DataFrame(records)\n",
        "\n",
        "# 7) Display\n",
        "# tools.display_dataframe_to_user(\"Flattened Judge Metrics\", df_gpt_judge)\n"
      ],
      "metadata": {
        "id": "TaBzZxUa13XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of score columns\n",
        "score_cols = [\n",
        "    'Coherence_and_Logic_score',\n",
        "    'Relevance_and_Accuracy_score',\n",
        "    'Readability_and_Style_score',\n",
        "    'Grammatical_Correctness_score',\n",
        "    'Overall_Impression_score'\n",
        "]\n",
        "\n",
        "# Print the average of each column\n",
        "for col in score_cols:\n",
        "    avg = df_gpt_judge21[col].mean()\n",
        "    print(avg)\n"
      ],
      "metadata": {
        "id": "Q5MpteBk13OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self Feedback"
      ],
      "metadata": {
        "id": "XJcTHmVv2g_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "futurw_work_generation_prompt = '''\n",
        "I want to generate future work directions for my research paper based on its entire content (all sections, including abstract, introduction,\n",
        "background, methodology, results, discussion, etc.). Please analyze the paper and propose substantial, long-term research goals\n",
        "that extend the current work in a meaningful way, advancing the field or addressing significant open challenges. Ensure the suggested\n",
        "future work directions are ambitious, grounded in the paper’s content, and avoid trivial or short-term tasks (e.g., minor experiments,\n",
        "parameter tuning, or small-scale tests). Each direction should be clearly linked to specific aspects of the paper (e.g., limitations,\n",
        "findings, or discussed challenges) and propose innovative, impactful research objectives. If no suitable long-term future work can\n",
        "be derived, clearly state: \"No long-term future work directions could be derived from the paper.\" Provide the generated future work\n",
        "directions in a concise, bulleted list, with each direction accompanied by a brief explanation of how it connects to the paper’s content.\n",
        "\n",
        "Input Text (Paper Content): [Insert the full text or relevant sections of the paper here]\n",
        "\n",
        "Output Format: Future Work Directions (Long-Term Goals)\n",
        "\n",
        "[Future work direction]: [Brief explanation of how this direction connects to the paper’s content and why it is a substantial,\n",
        "long-term goal.]\n",
        "\n",
        "[Additional future work directions and explanations, if applicable.]\n",
        "\n",
        "OR\n",
        "\n",
        "No long-term future work directions could be derived from the paper.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "aUByXm2617YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "import tiktoken\n",
        "import unicodedata\n",
        "\n",
        "# ── Setup & Helpers ────────────────────────────────────────────────────────────\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "enc     = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "MAX_CTX = 128_000\n",
        "\n",
        "def truncate_to_limit(text: str) -> str:\n",
        "    toks = enc.encode(text)\n",
        "    return enc.decode(toks[:MAX_CTX]) if len(toks) > MAX_CTX else text\n",
        "\n",
        "def to_ascii(text: str) -> str:\n",
        "    return unicodedata.normalize(\"NFD\", text).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "\n",
        "# ── New robust JSON parser with fallback ────────────────────────────────────────\n",
        "metrics = [\n",
        "    \"Coherence and Logic\",\n",
        "    \"Relevance and Accuracy\",\n",
        "    \"Readability and Style\",\n",
        "    \"Grammatical Correctness\",\n",
        "    \"Overall Impression\"\n",
        "]\n",
        "\n",
        "def parse_json_reply(snippet: str) -> dict:\n",
        "    \"\"\"Return a flat dict of *_score and *_justification (NaN if missing).\"\"\"\n",
        "    flat = {}\n",
        "    # initialize all to NaN\n",
        "    for m in metrics:\n",
        "        key = m.replace(\" \", \"_\")\n",
        "        flat[f\"{key}_score\"]         = np.nan\n",
        "        flat[f\"{key}_justification\"] = np.nan\n",
        "\n",
        "    # find JSON braces\n",
        "    start, end = snippet.find('{'), snippet.rfind('}')\n",
        "    if start < 0 or end < 0:\n",
        "        return flat\n",
        "\n",
        "    js = snippet[start:end+1]\n",
        "    # drop trailing commas\n",
        "    js = re.sub(r\",\\s*([}\\]])\", r\"\\1\", js)\n",
        "\n",
        "    try:\n",
        "        data = json.loads(js)\n",
        "    except json.JSONDecodeError:\n",
        "        return flat\n",
        "\n",
        "    # fill in parsed values\n",
        "    for m, info in data.items():\n",
        "        key = m.replace(\" \", \"_\")\n",
        "        flat[f\"{key}_score\"]         = info.get(\"score\", np.nan)\n",
        "        flat[f\"{key}_justification\"] = info.get(\"justification\", np.nan)\n",
        "    return flat\n",
        "\n",
        "# ── Identify your score/justification mapping ─────────────────────────────────\n",
        "score_cols = [c for c in df_rest.columns if c.startswith(\"fw_opr_\") and c.endswith(\"_score\")]\n",
        "just_cols  = {c: c.replace(\"_score\", \"_justification\") for c in score_cols}\n",
        "\n",
        "# ── Buffers for new columns ────────────────────────────────────────────────────\n",
        "regenerated_fw   = []\n",
        "evaluation_dicts = []\n",
        "\n",
        "# ── Main Loop ─────────────────────────────────────────────────────────────────\n",
        "for idx, row in df_rest.iterrows():\n",
        "    # 1) find any low scores ≤ 3\n",
        "    low = [\n",
        "        c for c in score_cols\n",
        "        if pd.to_numeric(row[c], errors=\"coerce\") <= 3\n",
        "    ]\n",
        "    if not low:\n",
        "        regenerated_fw.append(None)\n",
        "        evaluation_dicts.append({})  # empty dict → all NaNs\n",
        "        continue\n",
        "\n",
        "    # 2) bullet-list their justifications\n",
        "    bullets = \"\\n\".join(f\"- {row[just_cols[c]]}\" for c in low)\n",
        "\n",
        "    # 3) regeneration prompt\n",
        "    base_prompt = futurw_work_generation_prompt\n",
        "    inp_text    = row[\"Input_Text\"]\n",
        "    regen_p = (\n",
        "        \"The previous future work suggestions had these issues:\\n\"\n",
        "        f\"{bullets}\\n\\n\"\n",
        "        \"Please regenerate an improved set of future work suggestions \"\n",
        "        \"addressing these points, using this as your base prompt:\\n\\n\"\n",
        "        f\"{base_prompt}\\n\\n{inp_text}\"\n",
        "    )\n",
        "    regen_body = to_ascii(truncate_to_limit(regen_p))\n",
        "\n",
        "    # 4) stream regeneration\n",
        "    regen_text = \"\"\n",
        "    regen_stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": regen_body}],\n",
        "        stream=True,\n",
        "        temperature=0.1\n",
        "    )\n",
        "    for chunk in regen_stream:\n",
        "        regen_text += chunk.choices[0].delta.content or \"\"\n",
        "    regen_text = regen_text.strip()\n",
        "    regenerated_fw.append(regen_text)\n",
        "\n",
        "    # 5) evaluation prompt\n",
        "    author_str    = row[\"LLM_extracted_future_work\"]\n",
        "    eval_p = (\n",
        "        prompt\n",
        "        + \"Author-Mentioned Future Work:\\n\" + author_str\n",
        "        + \"\\n\\nLLM-Generated Future Work:\\n\" + regen_text\n",
        "    )\n",
        "    eval_body = to_ascii(truncate_to_limit(eval_p))\n",
        "\n",
        "    eval_text = \"\"\n",
        "    eval_stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": eval_body}],\n",
        "        stream=True,\n",
        "        temperature=0\n",
        "    )\n",
        "    for chunk in eval_stream:\n",
        "        eval_text += chunk.choices[0].delta.content or \"\"\n",
        "\n",
        "    # 6) parse JSON (with NaN fallback)\n",
        "    evaluation_dicts.append(parse_json_reply(eval_text))\n"
      ],
      "metadata": {
        "id": "tR5reGoP17pS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3085FfNb20pp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}