{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"dataset/df_data_preprocessed.csv\")\n"
      ],
      "metadata": {
        "id": "FW33geuNdIjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the average word count\n",
        "df['Word Count'] = pd.to_numeric(df['Word Count'], errors='coerce')\n",
        "\n",
        "average_word_count = df['Word Count'].mean()\n",
        "\n",
        "print(f\"Average Word Count: {average_word_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTHnQ0ACiM7G",
        "outputId": "d9d2477d-1d4d-4ed2-e8bb-dd6de60e7b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Word Count: 63.03200883002207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install transformers"
      ],
      "metadata": {
        "id": "s9SuOw5BpFFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['response_string'] = df.apply(lambda row: f\"\"\"Abstract: {row['Abstract']}\n",
        "Introduction: {row['Introduction']}\n",
        "Conclusion: {row['Conclusion']}\n",
        "Limitation: {row['Limitation']}\n",
        "Experiment_and_Results: {row['Experiment_and_Results']}\n",
        "Related_Work: {row['Related_Work']}\n",
        "Methodology: {row['Methodology']}\n",
        "Dataset: {row['Dataset']}\n",
        "\"\"\", axis=1)\n"
      ],
      "metadata": {
        "id": "HkcNzeeJpdNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmjg6HUZqO5b",
        "outputId": "84e0cf7a-d5c4-4f1f-93f2-dd07bae5cede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/386.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/325.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "generated_text = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    prompt = \"\"\"You are an AI trained to analyze scientific research and suggest future directions based on the content of a paper.\n",
        "    Below, you will find sections from a scientific article including the 'Abstract', 'Introduction', 'Conclusion','Limitation','Experiment and Results','Related Work','Methodology'\n",
        "    of a scientific paper.\n",
        "    Based on these details, please generate comprehensive and plausible future work suggestions that could extend the research findings,\n",
        "    address limitations, and propose new avenues for exploration.\n",
        "    Generate a future work based on these texts. Future work should be within 100 words. \\n\"\"\" + df['response_string'][i]\n",
        "    summary_text = \"\"  # Initialize an empty string to collect the limitation text\n",
        "\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        # model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0.2  # Adjust the temperature as needed, max_tokens=150\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        summary_chunk = chunk.choices[0].delta.content or \"\"\n",
        "        summary_text += summary_chunk\n",
        "\n",
        "    text_chunks = []\n",
        "    text_chunks.append(summary_text)\n",
        "    generated_text.append(text_chunks)\n"
      ],
      "metadata": {
        "id": "6lxkiQxFsyhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_generated_future_work = pd.DataFrame(generated_summary, columns=['Future_Work'])\n",
        "df['Future_Work'] = df_generated_future_work['Future_Work']"
      ],
      "metadata": {
        "id": "MR4_K0cvu_2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('dataset/df_gpt_3.csv', index=False)"
      ],
      "metadata": {
        "id": "FFINUDKL3ElX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}