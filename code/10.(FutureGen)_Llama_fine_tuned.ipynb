{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"dataset/df_data_preprocessed.csv\")"
      ],
      "metadata": {
        "id": "_mfnKVB1MoZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjJxw_v6MoZJ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (python 3.10/3.11 only)\n",
        "!pip install xformers\n"
      ],
      "metadata": {
        "id": "06drO1501WJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install peft"
      ],
      "metadata": {
        "id": "OaKID_k8MoZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after installing packages (above), restart the session\n",
        "# here they using flash attention, 4bit quantization\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "# max_seq_length = 14048 # tried with 14048 max sequence length but T4 and L4 can't take it\n",
        "# max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "max_seq_length = 2048 # change to 2048 tokens for limitations of GPU. Choose any! We auto support RoPE Scaling internally!\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    #\"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "ebOdXEgT1Yz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9A2wVjKMoZK"
      },
      "outputs": [],
      "source": [
        "# max_seq_length = 2048 # change to 2048 tokens for limitations of GPU. Choose any! We auto support RoPE Scaling internally!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a415a997-faee-4f7a-f560-f744e1b2d8da",
        "id": "vchyqR-dMoZK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.10.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install datasets"
      ],
      "metadata": {
        "id": "d5ff0hbdMoZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset formatting like huggingface\n",
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "# Format each row as a conversation\n",
        "formatted_data = [\n",
        "    [\n",
        "        {'from': 'human', 'value': row['Abstract'] + row['Introduction'] + row['Conclusion']},\n",
        "        {'from': 'gpt', 'value': row['GPT_ground_truth_FW']}\n",
        "    ]\n",
        "    for _, row in df_train.iterrows()\n",
        "]\n",
        "\n",
        "# Create a DataFrame and then convert to a Hugging Face Dataset\n",
        "df_conversations = pd.DataFrame({'conversations': formatted_data})\n",
        "huggingface_dataset = Dataset.from_pandas(df_conversations)\n",
        "\n"
      ],
      "metadata": {
        "id": "sGbfX8HzMoZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "huggingface_dataset = standardize_sharegpt(huggingface_dataset)\n",
        "huggingface_dataset = huggingface_dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "4612df9585354cc69919fc0ae5e7c4bd",
            "4b788f92c4db4569b031631876444ef3",
            "5b59abe82eb343d1beb2e3f0a71c6aa5",
            "c3c2b41359a84c11b4d224d3ead1ea66",
            "9a3068cd60914448a47d9814e657b6d7",
            "349fb6f0fa644072ab94b4a36e15412e",
            "d10bb594a9b2445893ede530e2cd1827",
            "877c12509fff4906a08d91e9d12f61f7",
            "c4b457226434415dbb2b103aa3f03b0d",
            "e21f8e16c86a4ae491bdcad23fbe314c",
            "cced062a7e044bca9ca8dacf68e1077f",
            "368b2a5ded924797adbb0d5f27527427",
            "a9d19e3856ad4016a9feea6aa6d32203",
            "60661d952f1242cba9acc618041b30b5",
            "5162773446494a25b632dad306494254",
            "ca268f54f60b41a89969002f093c98c2",
            "ed83b071a7674867a459772482d0042f",
            "9a705bfe7adf4ea1b3dd060d56d14a53",
            "6733f642e9af4a80a5a954223ae4991a",
            "6b830a2c4d1446ee8e28958c1ae60239",
            "1d23ac5e8d924d5eaa248ac68aa6ff3f",
            "70a18ca8be654dca8a3d77269606ff81"
          ]
        },
        "outputId": "073dd8f1-3eb0-4db8-fdbf-a6eb8f9e73fb",
        "id": "1QP-zYXhMoZN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Standardizing format:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4612df9585354cc69919fc0ae5e7c4bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "368b2a5ded924797adbb0d5f27527427"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = huggingface_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "9c227ab8211a4af7a185b24a6ea59774",
            "c714567ec6714f0e9e876bd68f2e556a",
            "012f9fd3f4034e4bb59039dba5c6b886",
            "c0e960da60364fd48d1499915a7df2c4",
            "65296f1906a145108bb5b2a0bcfce7e7",
            "668940b860984f949a74f6aa8ada4444",
            "b0f667f9f6d64240ab0bf7ba2990aa6f",
            "53f4a493bbfc4726962639af9bd9e98b",
            "4ea52f4dd6984ea1a8633cc982b195a2",
            "2dfc570f709d440f8e55387534d9050f",
            "4963a492acde48bc82387904ee295081"
          ]
        },
        "outputId": "831f40fa-7577-4d38-e76f-399f2a46d1ac",
        "id": "cmtnn3HSMoZP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c227ab8211a4af7a185b24a6ea59774"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6f4a99d0c3814fde9b2d4f8cf0314fb1",
            "e4e06be2e48842bfb577dd0d9c7e8573",
            "47dd90eabb5e4bdf9ff3a23b64cf2ad8",
            "56aab3c7badc490f9db0a4f1d6ef5e06",
            "3e481ee0326b4ff9978e4ef0f36bacef",
            "f7b45b9b597f4eac902b6f09c9336cf8",
            "90b5aff37421498fa94775f31102cde7",
            "ad2e1c6bcfde4fef9531b8f3a6c45402",
            "b9d90f16d0374910857f9852b001a778",
            "e20403d02d214021ab09df2ab3ae0285",
            "e556580f83de4613b3f20a8b17df905e"
          ]
        },
        "outputId": "c2612ee0-7061-4edf-f3a0-fc08c8eb60b4",
        "id": "wpZZ9ar_MoZR"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f4a99d0c3814fde9b2d4f8cf0314fb1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "hDmJCTlK1jun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'model' is your trained model and 'tokenizer' is your tokenizer\n",
        "model.save_pretrained('llama_fine_tuned_60_steps')\n",
        "tokenizer.save_pretrained('llama_fine_tuned_60_steps')\n"
      ],
      "metadata": {
        "id": "L7tQTsZQ1oIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "import torch\n",
        "\n",
        "# Initialize tokenizer with chat template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3.1\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Define the prompt template\n",
        "prompt_template = \"\"\"You are an AI trained to analyze scientific research and suggest future directions based on the content of a paper.\n",
        "Below, you will find sections from a scientific article including the 'Abstract', 'Introduction', 'Conclusion' of a scientific paper.\n",
        "Based on these details, please generate comprehensive and plausible future work suggestions that could extend the research findings,\n",
        "address limitations, and propose new avenues for exploration.\n",
        "Generate a future work based on these texts. Future work should be within 100 words.\\n\"\"\"\n",
        "\n",
        "# Initialize a list to store generated future work\n",
        "future_work = []\n",
        "\n",
        "# Loop through each row in the DataFrame\n",
        "for _, row in df_test.iterrows():\n",
        "    # Construct the user prompt using the specific row's content\n",
        "    prompt = (\n",
        "        prompt_template +\n",
        "        f\"Abstract: {row['Abstract']}\\nIntroduction: {row['Introduction']}\\nConclusion: {row['Conclusion']}\"\n",
        "    )\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "    # Extract input IDs and attention mask\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "   # Generate the output with new parameters\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=128,            # Increase max tokens to allow full sentence generation\n",
        "        use_cache=True,\n",
        "        temperature=1.0,               # Lower temperature for more focused output\n",
        "        top_p=0.9,                     # Use top-p sampling for better coherence\n",
        "        eos_token_id=tokenizer.eos_token_id,  # Set an end-of-sequence token\n",
        "        repetition_penalty=1.2         # Discourage abrupt/repetitive phrases\n",
        "    )\n",
        "\n",
        "    # Decode the output and add to the future_work list\n",
        "    generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
        "    future_work.append(generated_text)\n",
        "\n",
        "# Add the generated future work to the DataFrame\n",
        "df['generated_future_work'] = future_work\n",
        "\n"
      ],
      "metadata": {
        "id": "NStYect11wmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to refine the 'generated_future_work' column\n",
        "def refine_future_work(row):\n",
        "    # Extract the last sentence from the Conclusion\n",
        "    last_sentence = row['Conclusion'].split('.')[-2].strip() + '.'\n",
        "\n",
        "    # Split generated_future_work into sentences\n",
        "    future_work_sentences = row['generated_future_work'].split('. ')\n",
        "\n",
        "    # Reverse iterate to find the matching sentence\n",
        "    match_index = None\n",
        "    for i in range(len(future_work_sentences) - 1, -1, -1):\n",
        "        sentence = future_work_sentences[i].strip() + '.'\n",
        "        if sentence == last_sentence:\n",
        "            match_index = i\n",
        "            break\n",
        "\n",
        "    # If a match is found, keep sentences after the matched index\n",
        "    if match_index is not None:\n",
        "        refined_text = '. '.join(future_work_sentences[match_index + 1:])  # Keep sentences after the match\n",
        "    else:\n",
        "        refined_text = row['generated_future_work']  # If no match is found, keep original text\n",
        "\n",
        "    return refined_text\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_test['refine_generated_future_work'] = df_test.apply(refine_future_work, axis=1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C6btQ-y_MoZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.to_csv('df_llama2_ft.csv', index=False)  # Set index=False if you don't want to save the row indices"
      ],
      "metadata": {
        "id": "vt7HefmRMoZS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}