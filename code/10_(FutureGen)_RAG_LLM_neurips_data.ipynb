{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuxEoPpT0-2U"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_neurips = pd.read_csv(\"df_neurips.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "futurw_work_generation_prompt = '''\n",
        "I want to generate future work directions for my research paper based on its entire content (all sections, including abstract, introduction,\n",
        "background, methodology, results, discussion, etc.). Please analyze the paper and propose substantial, long-term research goals\n",
        "that extend the current work in a meaningful way, advancing the field or addressing significant open challenges. Ensure the suggested\n",
        "future work directions are ambitious, grounded in the paper’s content, and avoid trivial or short-term tasks (e.g., minor experiments,\n",
        "parameter tuning, or small-scale tests). Each direction should be clearly linked to specific aspects of the paper (e.g., limitations,\n",
        "findings, or discussed challenges) and propose innovative, impactful research objectives. If no suitable long-term future work can\n",
        "be derived, clearly state: \"No long-term future work directions could be derived from the paper.\" Provide the generated future work\n",
        "directions in a concise, bulleted list, with each direction accompanied by a brief explanation of how it connects to the paper’s content.\n",
        "\n",
        "Input Text (Paper Content): [Insert the full text or relevant sections of the paper here]\n",
        "\n",
        "Output Format: Future Work Directions (Long-Term Goals)\n",
        "\n",
        "[Future work direction]: [Brief explanation of how this direction connects to the paper’s content and why it is a substantial,\n",
        "long-term goal.]\n",
        "\n",
        "[Additional future work directions and explanations, if applicable.]\n",
        "\n",
        "OR\n",
        "\n",
        "No long-term future work directions could be derived from the paper.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "wyzDt74q1Q0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''\n",
        "Instructions: You are provided with two texts for each pair: one is Author-Mentioned Future Work and another is\n",
        "LLM-Generated Future Work. Please read both texts carefully. After reviewing each text, assign a score from\n",
        "1 to 5 for each criterion outlined below. The score should reflect how well the LLM-Generated Future Work compares to the\n",
        "Author-Mentioned Future Work,\n",
        "where 1 represents poor quality and 5 represents excellent quality that closely matches or even surpasses the Author-Mentioned Future Work\n",
        "in some aspects.\n",
        "\n",
        "Author-Mentioned Future Work:\n",
        "<Author-Mentioned Future Work>\n",
        "\n",
        "LLM-Generated Future Work:\n",
        "<LLM-Generated Future Work>\n",
        "\n",
        "Scoring Criteria:\n",
        "Coherence and Logic:\n",
        "\n",
        "5: The text is exceptionally coherent; the ideas flow logically and are well connected.\n",
        "\n",
        "3: The text is coherent but may have occasional lapses in logic or flow.\n",
        "\n",
        "1: The text is disjointed or frequently illogical.\n",
        "\n",
        "Relevance and Accuracy:\n",
        "\n",
        "5: The text is completely relevant to the topic and accurate in all presented facts.\n",
        "\n",
        "3: The text is generally relevant with minor factual errors or slight deviations from the topic.\n",
        "\n",
        "1: The text often strays off topic or includes multiple factual inaccuracies.\n",
        "\n",
        "Readability and Style:\n",
        "\n",
        "5: The text is engaging, well-written, and stylistically consistent with the Author-Mentioned Future Work\n",
        "3: The text is readable but may lack flair or have minor stylistic inconsistencies.\n",
        "1: The text is difficult to read or stylistically poor.\n",
        "\n",
        "Grammatical Correctness:\n",
        "\n",
        "5: The text is free from grammatical errors.\n",
        "3: The text has occasional grammatical errors that do not impede understanding.\n",
        "1: The text has frequent grammatical errors that hinder comprehension.\n",
        "\n",
        "Overall Impression:\n",
        "5: The text is of a quality that you would expect from a professional writer.\n",
        "3: The text is acceptable but would benefit from further editing.\n",
        "1: The text is of a quality that needs significant revision to be usable.\n",
        "\n",
        "Task: For each text pair:\n",
        "Rate the LLM-Generated Future Work on each criterion and provide a final overall score out of 5.\n",
        "Provide a justification for each criterion score, highlighting strengths and weaknesses observed in the LLM-Generated Future Work\n",
        "relative to the Author-Mentioned Future Work.\n",
        "Present the scores and justifications in JSON format, structured as follows:\n",
        "\n",
        "{ \"Coherence and Logic\": { \"score\": , \"justification\": \"\" }, \"Relevance and Accuracy\": { \"score\": , \"justification\": \"\" },\n",
        "\"Readability and Style\": { \"score\": , \"justification\": \"\" }, \"Grammatical Correctness\": { \"score\": , \"justification\": \"\" },\n",
        "\"Overall Impression\": { \"score\": , \"justification\": \"\" } }\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "Ze0V_Ep51RTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rest['response_string'] = df_rest.apply(\n",
        "    lambda row: (\n",
        "        f\"Abstract: {row['df_Abstract']}\\n\"\n",
        "        f\"Introduction: {row['df_Introduction']}\\n\"\n",
        "        f\"Related_Work: {row['df_Related_Work']}\\n\"\n",
        "        f\"Methodology: {row['df_Methodology']}\\n\"\n",
        "        f\"Dataset: {row['df_Dataset']}\\n\"\n",
        "        f\"Conclusion: {row['df_Conclusion']}\\n\"\n",
        "        f\"Experiment_and_Results: {row['df_Experiment_and_Results']}\\n\"\n",
        "        f\"Extra1: {row['df_col_2']}\\n\"\n",
        "        f\"Extra2: {row['df_col_3']}\\n\"\n",
        "        f\"Extra3: {row['df_col_4']}\\n\"\n",
        "        f\"Extra4: {row['df_col_5']}\\n\"\n",
        "        f\"Extra5: {row['df_col_6']}\\n\"\n",
        "        f\"Extra6: {row['df_col_7']}\\n\"\n",
        "        f\"Extra7: {row['df_col_8']}\"\n",
        "    ),\n",
        "    axis=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "mSBBb7-T1RJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tiktoken\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "\n",
        "# 1) Setup\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "chat_llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# token encoder & limit\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "MAX_TOKENS = 128_000\n",
        "\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "# 2) Build your ensemble retriever (same as before)\n",
        "hf_emb = OpenAIEmbeddings()\n",
        "\n",
        "def make_retriever_for_docs(docs, k=3):\n",
        "    # FAISS retriever\n",
        "    faiss_store = FAISS.from_documents(docs, hf_emb)\n",
        "    faiss_r = faiss_store.as_retriever(search_kwargs={\"k\": k})\n",
        "    # BM25 retriever\n",
        "    bm25_r = BM25Retriever.from_documents(docs)\n",
        "    bm25_r.k = k\n",
        "\n",
        "    # Must use keywords only here:\n",
        "    return EnsembleRetriever(\n",
        "        retrievers=[faiss_r, bm25_r],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "\n",
        "# load & convert your llama_index documents\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "documents = SimpleDirectoryReader(\"RAG_Data\").load_data()\n",
        "# lc_docs = [Document(page_content=d.text, metadata=getattr(d, \"metadata\", {}))\n",
        "#            for d in docs]\n",
        "\n",
        "# retriever = make_retriever_for_docs(lc_docs, k=3)\n",
        "\n",
        "# 1. After converting to LangChain Documents:\n",
        "lc_docs = [\n",
        "    Document(page_content=doc.text, metadata=doc.metadata or {})\n",
        "    for doc in documents\n",
        "]\n",
        "\n",
        "# 2. Chunk them with your desired token‐based size & overlap:\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512,      # max tokens per chunk\n",
        "    chunk_overlap=20     # overlap between chunks\n",
        ")\n",
        "chunked_docs = splitter.split_documents(lc_docs)\n",
        "\n",
        "# 3. Build your retriever on the chunked docs instead of the full ones:\n",
        "retriever = make_retriever_for_docs(chunked_docs, k=3)\n",
        "\n",
        "# 3) Loop with truncation logic\n",
        "generated_future_work = []\n",
        "for i in range(len(df_rest)): # len(df_rest)\n",
        "    # a) Build your user query\n",
        "    user_query = futurw_work_generation_prompt + df_rest.at[i, 'response_string']\n",
        "\n",
        "    # b) Fetch documents yourself\n",
        "    retrieved = retriever.get_relevant_documents(user_query)\n",
        "    docs_text = \"\\n\\n\".join([d.page_content for d in retrieved])\n",
        "\n",
        "    # c) Token counts\n",
        "    tok_docs  = enc.encode(docs_text)\n",
        "    tok_query = enc.encode(user_query)\n",
        "    total_len = len(tok_docs) + len(tok_query)\n",
        "\n",
        "    # d) If over limit, truncate **only** the query portion\n",
        "    if total_len > MAX_TOKENS:\n",
        "        allowed_for_query = MAX_TOKENS - len(tok_docs)\n",
        "        tok_query = tok_query[:allowed_for_query]\n",
        "        user_query = enc.decode(tok_query)\n",
        "\n",
        "    # e) Call the LLM with your “context + question” template\n",
        "#     system_prompt = \"\"\"You are an AI trained to analyze scientific research and suggest future directions...\n",
        "# Generate a future work summary within 100 words.\"\"\"\n",
        "\n",
        "    # Format a two‐message chat\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": futurw_work_generation_prompt + \"\\n\\nContext:\\n\" + docs_text},\n",
        "        {\"role\": \"user\",   \"content\": user_query}\n",
        "    ]\n",
        "\n",
        "    # resp = llm.chat(messages=messages)\n",
        "    # Option A: call the model directly\n",
        "    # resp = llm(messages=messages)\n",
        "    response = chat_llm.invoke(messages)\n",
        "    generated_text = response.content  # this is the AI’s reply\n",
        "    generated_future_work.append(generated_text)\n",
        "    # generated_future_work.append(resp.choices[0].message.content)\n",
        "\n",
        "# 4) Attach back to DataFrame\n",
        "df_rest['Generated_future_work_rag_llm'] = generated_future_work\n"
      ],
      "metadata": {
        "id": "ItkyVJz41YcT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}